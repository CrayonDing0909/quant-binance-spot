#!/usr/bin/env python3
"""
R3C Universe â€” Final Blocking Gate Check (Pre-Live)
=====================================================

5 mandatory gates â€” any FAIL â†’ NO_GO:
  A1) Overlay-inclusive Walk-Forward (5 splits)
  A2) Time alignment / look-ahead check (truncation invariance)
  A3) Online vs Offline consistency
  A4) Execution parity (same-bar violations = 0)
  A5) +1 bar delay stress

Rules:
  - NO parameter changes (R3C config as-is)
  - NO strategy logic changes
  - Cost model ON (fee/slippage/funding)
  - trade_on=next_open maintained
  - Any single gate FAIL â†’ NO_GO

Usage:
    cd /path/to/quant-binance-spot
    PYTHONPATH=src python scripts/run_r3c_universe_final_gate.py
"""
from __future__ import annotations

import json
import logging
import sys
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import yaml

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import vectorbt as vbt

from qtrade.config import load_config
from qtrade.backtest.run_backtest import (
    _bps_to_pct,
    to_vbt_direction,
    clip_positions_by_direction,
    _apply_date_filter,
)
from qtrade.backtest.costs import (
    compute_funding_costs,
    adjust_equity_for_funding,
    compute_adjusted_stats,
)
from qtrade.strategy import get_strategy
from qtrade.strategy.base import StrategyContext
from qtrade.data.storage import load_klines
from qtrade.data.quality import clean_data
from qtrade.data.funding_rate import (
    load_funding_rates,
    get_funding_rate_path,
    align_funding_to_klines,
)
from qtrade.strategy.overlays.oi_vol_exit_overlay import (
    apply_overlay_by_mode,
    compute_flip_count,
)
from qtrade.strategy.overlays.microstructure_accel_overlay import (
    apply_full_micro_accel_overlay,
    compute_micro_features,
    compute_accel_score,
    apply_accel_overlay,
    load_multi_tf_klines,
)

logging.basicConfig(level=logging.WARNING, format="%(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
warnings.filterwarnings("ignore", category=UserWarning)

PROJECT_ROOT = Path(__file__).parent.parent

# â”€â”€ Production config (R3C Universe) â”€â”€
CONFIG_PATH = PROJECT_ROOT / "config" / "prod_candidate_R3C_universe.yaml"

# 19-symbol universe
SYMBOLS = [
    "BTCUSDT", "ETHUSDT", "SOLUSDT", "BNBUSDT", "XRPUSDT",
    "DOGEUSDT", "ADAUSDT", "AVAXUSDT", "LINKUSDT", "DOTUSDT",
    "LTCUSDT", "NEARUSDT", "APTUSDT", "ARBUSDT", "OPUSDT",
    "FILUSDT", "ATOMUSDT", "UNIUSDT", "AAVEUSDT",
]

WEIGHTS = {
    "BTCUSDT": 0.0722, "ETHUSDT": 0.0538, "SOLUSDT": 0.0509,
    "BNBUSDT": 0.0707, "XRPUSDT": 0.0491, "DOGEUSDT": 0.0512,
    "ADAUSDT": 0.0511, "AVAXUSDT": 0.0545, "LINKUSDT": 0.0538,
    "DOTUSDT": 0.0540, "LTCUSDT": 0.0605, "NEARUSDT": 0.0495,
    "APTUSDT": 0.0483, "ARBUSDT": 0.0495, "OPUSDT": 0.0426,
    "FILUSDT": 0.0489, "ATOMUSDT": 0.0523, "UNIUSDT": 0.0394,
    "AAVEUSDT": 0.0477,
}

# Key symbols that must pass WF individually
KEY_SYMBOLS = ["BTCUSDT", "ETHUSDT", "SOLUSDT"]

# Known auto_delay=False strategies
AUTO_DELAY_FALSE = {
    "breakout_vol_atr", "breakout_vol", "nw_envelope_regime",
    "rsi_adx_atr", "mr_bollinger", "mr_zscore",
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Config helpers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _load_ensemble_strategy(config_path: str | Path, symbol: str):
    with open(config_path, "r", encoding="utf-8") as f:
        raw = yaml.safe_load(f)
    ens = raw.get("ensemble")
    if ens and ens.get("enabled", False):
        strategies = ens.get("strategies", {})
        if symbol in strategies:
            s = strategies[symbol]
            return s["name"], s.get("params", {})
    return None


def _load_micro_accel_params(config_path: str | Path) -> dict | None:
    with open(config_path, "r", encoding="utf-8") as f:
        raw = yaml.safe_load(f)
    micro = raw.get("strategy", {}).get("micro_accel_overlay")
    if micro and micro.get("enabled", False):
        return micro.get("params", {})
    return None


def _load_vol_overlay_params(config_path: str | Path) -> dict | None:
    with open(config_path, "r", encoding="utf-8") as f:
        raw = yaml.safe_load(f)
    overlay = raw.get("strategy", {}).get("overlay")
    if overlay and overlay.get("enabled", False):
        return overlay
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Core: Single symbol full pipeline (with both overlays)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _run_symbol_full(
    symbol: str,
    cfg,
    config_path: str | Path,
    cost_mult: float = 1.0,
    micro_accel_params: dict | None = None,
    start_override: str | None = None,
    end_override: str | None = None,
    extra_signal_delay: int = 0,
) -> dict | None:
    """Run single-symbol with both vol and micro overlays, returning full details."""
    market_type = cfg.market_type_str
    data_path = (
        cfg.data_dir / "binance" / market_type
        / cfg.market.interval / f"{symbol}.parquet"
    )
    if not data_path.exists():
        return None

    df = load_klines(data_path)
    df = clean_data(df, fill_method="forward", remove_outliers=False,
                    remove_duplicates=True)

    multi_tf = load_multi_tf_klines(cfg.data_dir, symbol, market_type)
    df_5m = multi_tf.get("5m")
    df_15m = multi_tf.get("15m")

    # Resolve strategy (ensemble routing)
    ensemble_override = _load_ensemble_strategy(config_path, symbol)
    if ensemble_override:
        strategy_name, strategy_params = ensemble_override
    else:
        strategy_name = cfg.strategy.name
        strategy_params = cfg.strategy.get_params(symbol)

    # Build context
    total_delay = 1 + extra_signal_delay
    ctx = StrategyContext(
        symbol=symbol,
        interval=cfg.market.interval,
        market_type=market_type,
        direction=cfg.direction,
        signal_delay=total_delay,
    )

    # Generate base positions
    strategy_func = get_strategy(strategy_name)
    pos_base = strategy_func(df, ctx, strategy_params)

    # Apply vol overlay (R2.1)
    pos_after_vol = pos_base.copy()
    vol_overlay = _load_vol_overlay_params(config_path)
    if vol_overlay and vol_overlay.get("enabled", False):
        overlay_mode = vol_overlay.get("mode", "vol_pause")
        overlay_params = vol_overlay.get("params", {})
        pos_after_vol = apply_overlay_by_mode(
            position=pos_after_vol,
            price_df=df,
            oi_series=None,
            params=overlay_params,
            mode=overlay_mode,
        )

    # Apply micro accel overlay (R3 Track A)
    pos_after_micro = pos_after_vol.copy()
    if micro_accel_params is not None:
        oi_series = None
        try:
            from qtrade.data.open_interest import (
                get_oi_path, load_open_interest, align_oi_to_klines,
            )
            for prov in ["merged", "binance"]:
                oi_path = get_oi_path(cfg.data_dir, symbol, prov)
                oi_df = load_open_interest(oi_path)
                if oi_df is not None and not oi_df.empty:
                    oi_series = align_oi_to_klines(oi_df, df.index, max_ffill_bars=2)
                    break
        except Exception:
            pass

        pos_after_micro = apply_full_micro_accel_overlay(
            base_position=pos_after_vol,
            df_1h=df,
            df_5m=df_5m,
            df_15m=df_15m,
            oi_series=oi_series,
            params=micro_accel_params,
        )

    pos = pos_after_micro

    # Direction clip
    pos = clip_positions_by_direction(pos, market_type, cfg.direction)

    # Position sizing
    ps_cfg = cfg.position_sizing
    if ps_cfg.method == "fixed" and ps_cfg.position_pct < 1.0:
        pos = pos * ps_cfg.position_pct

    # Date filter
    start = start_override or cfg.market.start
    end = end_override or cfg.market.end
    df, pos = _apply_date_filter(df, pos, start, end)

    if len(df) < 100:
        return None

    # Cost with multiplier
    fee_bps = cfg.backtest.fee_bps * cost_mult
    slippage_bps = cfg.backtest.slippage_bps * cost_mult
    fee = _bps_to_pct(fee_bps)
    slippage = _bps_to_pct(slippage_bps)
    initial_cash = cfg.backtest.initial_cash

    close = df["close"]
    open_ = df["open"]

    vbt_direction = to_vbt_direction(cfg.direction)
    pf = vbt.Portfolio.from_orders(
        close=close,
        size=pos,
        size_type="targetpercent",
        price=open_,
        fees=fee,
        slippage=slippage,
        init_cash=initial_cash,
        freq=cfg.market.interval,
        direction=vbt_direction,
    )

    stats = pf.stats()
    equity = pf.value()

    # Funding rate
    adjusted_equity = None
    adjusted_stats = None
    funding_cost_total = 0.0
    fr = cfg.backtest.funding_rate
    if fr.enabled and market_type == "futures":
        funding_df = None
        if fr.use_historical:
            fr_path = get_funding_rate_path(cfg.data_dir, symbol)
            funding_df = load_funding_rates(fr_path)

        funding_rates = align_funding_to_klines(
            funding_df, df.index,
            default_rate_8h=fr.default_rate_8h,
        )
        leverage = cfg.futures.leverage if cfg.futures else 1
        fc = compute_funding_costs(
            pos=pos, equity=equity,
            funding_rates=funding_rates,
            leverage=leverage,
        )
        adjusted_equity = adjust_equity_for_funding(equity, fc)
        adjusted_stats = compute_adjusted_stats(adjusted_equity, initial_cash)
        funding_cost_total = fc.total_cost

    final_stats = adjusted_stats if adjusted_stats else stats
    eq = adjusted_equity if adjusted_equity is not None else equity

    total_return_pct = final_stats.get("Total Return [%]", 0.0)
    sharpe = final_stats.get("Sharpe Ratio", 0.0)
    max_dd = abs(final_stats.get("Max Drawdown [%]", 0.0))
    total_trades = stats.get("Total Trades", 0)

    n_bars = len(df)
    years = n_bars / (365.25 * 24)
    total_ret = total_return_pct / 100.0
    cagr = ((1 + total_ret) ** (1 / max(years, 0.01)) - 1) * 100 if years > 0 else 0
    calmar = cagr / max_dd if max_dd > 0.01 else 0
    flips = compute_flip_count(pos)

    pos_changes = pos.diff().abs().fillna(0)
    turnover = float(pos_changes.sum())

    return {
        "symbol": symbol,
        "total_return_pct": total_return_pct,
        "sharpe": sharpe,
        "max_drawdown_pct": max_dd,
        "total_trades": int(total_trades),
        "cagr": cagr,
        "calmar": calmar,
        "flips": flips,
        "equity": eq,
        "pos": pos,
        "pos_base": pos_base,
        "pos_after_vol": pos_after_vol,
        "pos_after_micro": pos_after_micro,
        "turnover": turnover,
        "funding_cost_abs": funding_cost_total,
        "n_bars": n_bars,
        "initial_cash": initial_cash,
        "df": df,
    }


def _run_portfolio(
    cfg, config_path, cost_mult=1.0, micro_accel_params=None,
    start_override=None, end_override=None, extra_signal_delay=0,
    symbols=None, weights=None,
) -> dict:
    """Run portfolio and aggregate."""
    use_symbols = symbols or SYMBOLS
    use_weights = weights or WEIGHTS
    per_symbol = {}

    for symbol in use_symbols:
        try:
            res = _run_symbol_full(
                symbol, cfg, config_path, cost_mult, micro_accel_params,
                start_override, end_override, extra_signal_delay,
            )
            if res is not None:
                per_symbol[symbol] = res
        except Exception as e:
            logger.warning(f"    âš ï¸  {symbol} failed: {e}")

    if not per_symbol:
        return {
            "sharpe": 0, "total_return_pct": 0, "max_drawdown_pct": 0,
            "cagr": 0, "calmar": 0, "total_trades": 0, "flips": 0,
            "per_symbol": {},
        }

    initial_cash = cfg.backtest.initial_cash
    active = list(per_symbol.keys())
    w = np.array([use_weights.get(s, 1.0 / len(use_symbols)) for s in active])
    w = w / w.sum()

    eqs = {s: per_symbol[s]["equity"] for s in active}
    min_start = max(eq.index[0] for eq in eqs.values())
    max_end = min(eq.index[-1] for eq in eqs.values())
    for s in active:
        eqs[s] = eqs[s].loc[min_start:max_end]

    norm = {}
    for s in active:
        eq = eqs[s]
        if len(eq) > 0 and eq.iloc[0] > 0:
            norm[s] = eq / eq.iloc[0]
        else:
            norm[s] = pd.Series(1.0, index=eq.index)

    port_norm = sum(norm[s] * wi for s, wi in zip(active, w))
    port_eq = port_norm * initial_cash
    port_ret = port_eq.pct_change().fillna(0)

    n_bars = len(port_ret)
    years = n_bars / (365.25 * 24)
    total_return = (port_eq.iloc[-1] / initial_cash - 1) * 100
    cagr = ((1 + total_return / 100) ** (1 / max(years, 0.01)) - 1) * 100

    rolling_max = port_eq.expanding().max()
    dd = (port_eq - rolling_max) / rolling_max
    max_dd = abs(dd.min()) * 100

    sharpe = (
        np.sqrt(365 * 24) * port_ret.mean() / port_ret.std()
        if port_ret.std() > 0 else 0
    )
    calmar = cagr / max_dd if max_dd > 0.01 else 0

    total_trades = sum(per_symbol[s]["total_trades"] for s in active)
    total_flips = sum(per_symbol[s]["flips"] for s in active)

    return {
        "total_return_pct": round(total_return, 2),
        "cagr": round(cagr, 2),
        "sharpe": round(sharpe, 2),
        "max_drawdown_pct": round(max_dd, 2),
        "calmar": round(calmar, 2),
        "total_trades": total_trades,
        "flips": total_flips,
        "per_symbol": per_symbol,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# A1: Overlay-inclusive Walk-Forward (5 splits)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def gate_a1_overlay_inclusive_wf(
    cfg, config_path, micro_accel_params, n_splits=5,
    symbols=None,
) -> dict:
    """
    Walk-Forward with BOTH overlays applied in each split.
    CRITICAL: Uses _run_symbol_full() which includes micro accel overlay.
    """
    print("\n  [A1] Overlay-inclusive Walk-Forward")
    use_symbols = symbols or SYMBOLS
    results = {}

    for symbol in use_symbols:
        market_type = cfg.market_type_str
        data_path = (
            cfg.data_dir / "binance" / market_type
            / cfg.market.interval / f"{symbol}.parquet"
        )
        if not data_path.exists():
            results[symbol] = {"oos_sharpes": [], "oos_returns": [], "error": "no data"}
            continue

        df_raw = load_klines(data_path)

        # Apply config date range FIRST, then split
        cfg_start = cfg.market.start
        if cfg_start:
            start_ts = pd.Timestamp(cfg_start)
            if df_raw.index.tz is not None:
                start_ts = start_ts.tz_localize(df_raw.index.tz)
            df_raw = df_raw[df_raw.index >= start_ts]

        cfg_end = cfg.market.end
        if cfg_end:
            end_ts = pd.Timestamp(cfg_end)
            if df_raw.index.tz is not None:
                end_ts = end_ts.tz_localize(df_raw.index.tz)
            df_raw = df_raw[df_raw.index <= end_ts]

        total_len = len(df_raw)
        if total_len < 1000:
            results[symbol] = {"oos_sharpes": [], "oos_returns": [], "error": "insufficient data"}
            continue

        n_segments = n_splits + 1
        seg_len = total_len // n_segments
        if seg_len < 500:
            n_segments = max(2, total_len // 500)
            n_splits_adj = n_segments - 1
            seg_len = total_len // n_segments
        else:
            n_splits_adj = n_splits

        print(f"    {symbol}: {total_len:,} bars, {n_splits_adj} splits, ~{seg_len:,} bars/seg")

        oos_sharpes = []
        oos_returns = []
        is_sharpes = []

        for i in range(n_splits_adj):
            train_end = seg_len * (i + 1)
            test_start = train_end
            test_end = min(seg_len * (i + 2), total_len)

            if test_end - test_start < 200:
                break

            test_start_date = df_raw.index[test_start].strftime("%Y-%m-%d")
            test_end_date = df_raw.index[test_end - 1].strftime("%Y-%m-%d")
            train_start_date = df_raw.index[0].strftime("%Y-%m-%d")
            train_end_date = df_raw.index[train_end - 1].strftime("%Y-%m-%d")

            # IS: full training window
            is_res = _run_symbol_full(
                symbol, cfg, config_path, cost_mult=1.0,
                micro_accel_params=micro_accel_params,
                start_override=train_start_date,
                end_override=train_end_date,
            )
            # OOS: test window only
            oos_res = _run_symbol_full(
                symbol, cfg, config_path, cost_mult=1.0,
                micro_accel_params=micro_accel_params,
                start_override=test_start_date,
                end_override=test_end_date,
            )

            is_sr = is_res["sharpe"] if is_res else 0.0
            oos_sr = oos_res["sharpe"] if oos_res else 0.0
            oos_ret = oos_res["total_return_pct"] if oos_res else 0.0

            is_sharpes.append(is_sr)
            oos_sharpes.append(oos_sr)
            oos_returns.append(oos_ret)

            icon = "âœ…" if oos_sr > 0 else "âŒ"
            print(
                f"      Split {i+1}: IS SR={is_sr:.2f}, "
                f"OOS SR={oos_sr:.2f}, OOS Ret={oos_ret:+.1f}% {icon}"
            )

        n_oos_pos = sum(1 for s in oos_sharpes if s > 0)
        avg_oos_sr = np.mean(oos_sharpes) if oos_sharpes else 0.0
        avg_is_sr = np.mean(is_sharpes) if is_sharpes else 0.0

        results[symbol] = {
            "is_sharpes": [round(s, 3) for s in is_sharpes],
            "oos_sharpes": [round(s, 3) for s in oos_sharpes],
            "oos_returns": [round(r, 2) for r in oos_returns],
            "n_oos_positive": n_oos_pos,
            "n_splits": len(oos_sharpes),
            "avg_oos_sharpe": round(avg_oos_sr, 3),
            "avg_is_sharpe": round(avg_is_sr, 3),
        }

    # Aggregate: portfolio-level OOS Sharpe
    # Run WF on portfolio too (combine per-symbol equity in each split)
    total_oos_pos = sum(r.get("n_oos_positive", 0) for r in results.values())
    total_splits = sum(r.get("n_splits", 0) for r in results.values())
    all_oos_sharpes = []
    for r in results.values():
        all_oos_sharpes.extend(r.get("oos_sharpes", []))
    avg_oos = np.mean(all_oos_sharpes) if all_oos_sharpes else 0

    # Key symbols check
    key_results = {}
    for ks in KEY_SYMBOLS:
        if ks in results:
            r = results[ks]
            key_results[ks] = {
                "n_oos_positive": r.get("n_oos_positive", 0),
                "n_splits": r.get("n_splits", 0),
                "avg_oos_sharpe": r.get("avg_oos_sharpe", 0),
            }

    return {
        "per_symbol": results,
        "total_oos_positive": total_oos_pos,
        "total_splits": total_splits,
        "avg_oos_sharpe": round(avg_oos, 3),
        "oos_positive_ratio": f"{total_oos_pos}/{total_splits}",
        "key_symbols": key_results,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# A2: Time alignment / look-ahead check + truncation invariance
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def gate_a2_time_alignment(
    cfg, config_path, micro_accel_params,
    symbols=None,
) -> dict:
    """
    1. Check 5m/15m feature aggregation to 1h uses only completed bars.
    2. Truncation invariance: full vs 80%-tail position consistency.
    """
    print("\n  [A2] Time Alignment / Look-ahead Check")
    use_symbols = symbols or ["BTCUSDT", "ETHUSDT", "SOLUSDT", "BNBUSDT", "LINKUSDT"]
    results = {}
    market_type = cfg.market_type_str

    for symbol in use_symbols:
        print(f"    {symbol}:")

        # â”€â”€ Part A: 5m/15m â†’ 1h timestamp alignment â”€â”€
        data_path = (
            cfg.data_dir / "binance" / market_type
            / cfg.market.interval / f"{symbol}.parquet"
        )
        if not data_path.exists():
            results[symbol] = {"pass": False, "reason": "no data"}
            continue

        df_1h = load_klines(data_path)
        df_1h = clean_data(df_1h, fill_method="forward",
                           remove_outliers=False, remove_duplicates=True)

        multi_tf = load_multi_tf_klines(cfg.data_dir, symbol, market_type)
        df_5m = multi_tf.get("5m")
        df_15m = multi_tf.get("15m")

        # Check: after resample, no future timestamps
        # The 5m bar at e.g. 14:55 should map to 14:00 1h bar (label='left', closed='left')
        # meaning the 14:00 1h bar sees 14:00-14:55 5m data = all completed in [14:00, 15:00)
        future_violations = 0
        if df_5m is not None and len(df_5m) > 0:
            # Compute a dummy feature and check alignment
            test_feature = df_5m["close"].pct_change().rolling(12, min_periods=6).std()
            resampled = test_feature.resample("1h", label="left", closed="left").last()

            # Each resampled timestamp should be <= the latest 5m timestamp that contributes
            # i.e., 1h bar at T:00 should only use 5m data from [T:00, T+1:00)
            # which is completed by T+1:00, so it's available at T+1:00
            # The signal for bar T:00 is used with shift(1), executed at T+1:00 open âœ“

            # Check: no NaN-introduced future leaks
            aligned = resampled.reindex(df_1h.index)
            # Each aligned[t] should only use data up to t+1h
            # Since we use label='left', closed='left': yes, 1h bar at t uses [t, t+1h) sub-bars
            # This is correct. Just verify no timestamp anomalies.
            if len(resampled) > 0 and len(df_1h) > 0:
                max_5m_ts = df_5m.index[-1]
                max_resampled_ts = resampled.index[-1]
                if max_resampled_ts > max_5m_ts:
                    future_violations += 1
                    print(f"      âš ï¸  Resampled timestamp {max_resampled_ts} > last 5m {max_5m_ts}")

            print(f"      5mâ†’1h alignment: {'âœ…' if future_violations == 0 else 'âŒ'} "
                  f"({len(df_5m):,} 5m bars â†’ {len(resampled):,} 1h bars, "
                  f"future violations={future_violations})")

        # â”€â”€ Part B: Truncation invariance â”€â”€
        # Full run
        full_res = _run_symbol_full(
            symbol, cfg, config_path, cost_mult=1.0,
            micro_accel_params=micro_accel_params,
        )
        if full_res is None:
            results[symbol] = {"pass": False, "reason": "full run failed"}
            continue

        # Get 80% tail cutoff
        n = len(df_1h)
        cutoff_idx = int(n * 0.20)
        if cutoff_idx >= n - 200:
            cutoff_idx = max(0, n - 500)
        cutoff_date = df_1h.index[cutoff_idx].strftime("%Y-%m-%d")

        trunc_res = _run_symbol_full(
            symbol, cfg, config_path, cost_mult=1.0,
            micro_accel_params=micro_accel_params,
            start_override=cutoff_date,
        )
        if trunc_res is None:
            results[symbol] = {"pass": False, "reason": "truncated too short"}
            continue

        # Compare positions in overlap region
        full_pos = full_res["pos"]
        trunc_pos = trunc_res["pos"]

        # Find common index
        common_idx = full_pos.index.intersection(trunc_pos.index)
        if len(common_idx) < 100:
            results[symbol] = {"pass": False, "reason": "overlap too short"}
            continue

        full_common = full_pos.reindex(common_idx)
        trunc_common = trunc_pos.reindex(common_idx)

        # Mismatch: where positions differ by > tiny epsilon
        diff = (full_common - trunc_common).abs()
        mismatch_count = int((diff > 0.001).sum())
        mismatch_ratio = mismatch_count / len(common_idx)

        # Also compare Sharpe
        full_sr = full_res["sharpe"]
        trunc_sr = trunc_res["sharpe"]
        sr_diff = abs(full_sr - trunc_sr) / max(abs(full_sr), 0.01)

        passed = mismatch_ratio < 0.05 and sr_diff < 0.30  # allow small diffs
        icon = "âœ…" if passed else "âŒ"
        print(f"      Truncation: {icon} mismatch={mismatch_count}/{len(common_idx)} "
              f"({mismatch_ratio:.2%}), SR: full={full_sr:.2f} trunc={trunc_sr:.2f} "
              f"(diff={sr_diff:.2%})")

        results[symbol] = {
            "pass": passed,
            "future_violations": future_violations,
            "mismatch_count": mismatch_count,
            "mismatch_ratio": round(mismatch_ratio, 4),
            "overlap_bars": len(common_idx),
            "full_sharpe": round(full_sr, 3),
            "trunc_sharpe": round(trunc_sr, 3),
            "sharpe_diff_pct": round(sr_diff * 100, 1),
        }

    all_pass = all(
        r.get("pass", False)
        for r in results.values()
        if isinstance(r, dict)
    )

    return {"per_symbol": results, "all_pass": all_pass}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# A3: Online vs Offline consistency
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def gate_a3_online_offline(
    cfg, config_path, micro_accel_params,
    symbols=None,
) -> dict:
    """
    Compare bar-by-bar online re-computation vs full offline computation.
    Simulates live: for each bar i, compute signal using [0, i] data only.
    """
    print("\n  [A3] Online vs Offline Consistency")
    use_symbols = symbols or ["BTCUSDT", "ETHUSDT", "SOLUSDT"]
    results = {}
    market_type = cfg.market_type_str

    for symbol in use_symbols:
        print(f"    {symbol}:")

        # Full offline run
        offline_res = _run_symbol_full(
            symbol, cfg, config_path, cost_mult=1.0,
            micro_accel_params=micro_accel_params,
        )
        if offline_res is None:
            results[symbol] = {"pass": False, "reason": "offline run failed"}
            continue

        offline_pos = offline_res["pos"]

        # "Online" simulation: run on expanding windows of the last 500 bars
        # (full online for all bars would be too slow â€” use spot checks)
        n = len(offline_pos)
        check_points = list(range(max(500, n - 200), n, 10))  # check every 10th bar near end
        if not check_points:
            check_points = [n - 1]

        # Strategy parameters
        data_path = (
            cfg.data_dir / "binance" / market_type
            / cfg.market.interval / f"{symbol}.parquet"
        )
        df_full = load_klines(data_path)
        df_full = clean_data(df_full, fill_method="forward",
                             remove_outliers=False, remove_duplicates=True)

        # Apply config date filter
        cfg_start = cfg.market.start
        if cfg_start:
            start_ts = pd.Timestamp(cfg_start)
            if df_full.index.tz is not None:
                start_ts = start_ts.tz_localize(df_full.index.tz)
            df_full = df_full[df_full.index >= start_ts]

        cfg_end = cfg.market.end
        if cfg_end:
            end_ts = pd.Timestamp(cfg_end)
            if df_full.index.tz is not None:
                end_ts = end_ts.tz_localize(df_full.index.tz)
            df_full = df_full[df_full.index <= end_ts]

        multi_tf = load_multi_tf_klines(cfg.data_dir, symbol, market_type)
        df_5m = multi_tf.get("5m")
        df_15m = multi_tf.get("15m")

        ensemble_override = _load_ensemble_strategy(config_path, symbol)
        if ensemble_override:
            strategy_name, strategy_params = ensemble_override
        else:
            strategy_name = cfg.strategy.name
            strategy_params = cfg.strategy.get_params(symbol)

        # Recompute the full offline signal chain manually
        ctx = StrategyContext(
            symbol=symbol,
            interval=cfg.market.interval,
            market_type=market_type,
            direction=cfg.direction,
            signal_delay=1,
        )
        strategy_func = get_strategy(strategy_name)
        pos_full_base = strategy_func(df_full, ctx, strategy_params)

        # Apply vol overlay
        vol_overlay = _load_vol_overlay_params(config_path)
        pos_full_after_vol = pos_full_base.copy()
        if vol_overlay and vol_overlay.get("enabled", False):
            overlay_mode = vol_overlay.get("mode", "vol_pause")
            overlay_params_dict = vol_overlay.get("params", {})
            pos_full_after_vol = apply_overlay_by_mode(
                position=pos_full_after_vol,
                price_df=df_full,
                oi_series=None,
                params=overlay_params_dict,
                mode=overlay_mode,
            )

        # Apply micro overlay
        pos_full_after_micro = pos_full_after_vol.copy()
        if micro_accel_params is not None:
            oi_series = None
            try:
                from qtrade.data.open_interest import (
                    get_oi_path, load_open_interest, align_oi_to_klines,
                )
                for prov in ["merged", "binance"]:
                    oi_path = get_oi_path(cfg.data_dir, symbol, prov)
                    oi_df = load_open_interest(oi_path)
                    if oi_df is not None and not oi_df.empty:
                        oi_series = align_oi_to_klines(oi_df, df_full.index, max_ffill_bars=2)
                        break
            except Exception:
                pass

            pos_full_after_micro = apply_full_micro_accel_overlay(
                base_position=pos_full_after_vol,
                df_1h=df_full,
                df_5m=df_5m,
                df_15m=df_15m,
                oi_series=oi_series,
                params=micro_accel_params,
            )

        pos_full_final = clip_positions_by_direction(
            pos_full_after_micro, market_type, cfg.direction
        )

        # Compare: truncated computation vs full
        # Run on 80% and 60% of data, compare positions in the common region
        n_full = len(df_full)
        matches = 0
        total_checks = 0

        for trunc_pct in [0.6, 0.8]:
            trunc_n = int(n_full * trunc_pct)
            if trunc_n < 200:
                continue

            df_trunc = df_full.iloc[:trunc_n]
            df_5m_trunc = None
            df_15m_trunc = None
            if df_5m is not None:
                end_ts = df_trunc.index[-1]
                df_5m_trunc = df_5m[df_5m.index <= end_ts]
            if df_15m is not None:
                end_ts = df_trunc.index[-1]
                df_15m_trunc = df_15m[df_15m.index <= end_ts]

            pos_trunc = strategy_func(df_trunc, ctx, strategy_params)

            if vol_overlay and vol_overlay.get("enabled", False):
                pos_trunc = apply_overlay_by_mode(
                    position=pos_trunc, price_df=df_trunc, oi_series=None,
                    params=overlay_params_dict, mode=overlay_mode,
                )

            if micro_accel_params is not None:
                oi_trunc = None
                if oi_series is not None:
                    oi_trunc = oi_series.reindex(df_trunc.index).ffill()

                pos_trunc = apply_full_micro_accel_overlay(
                    base_position=pos_trunc, df_1h=df_trunc,
                    df_5m=df_5m_trunc, df_15m=df_15m_trunc,
                    oi_series=oi_trunc, params=micro_accel_params,
                )

            pos_trunc = clip_positions_by_direction(pos_trunc, market_type, cfg.direction)

            # Compare in the overlapping region
            common = pos_full_final.index[:trunc_n]
            full_vals = pos_full_final.reindex(common)
            trunc_vals = pos_trunc.reindex(common)

            diff = (full_vals - trunc_vals).abs()
            match_count = int((diff <= 0.001).sum())
            matches += match_count
            total_checks += len(common)

        consistency = matches / max(total_checks, 1) * 100
        passed = consistency >= 99.9
        icon = "âœ…" if passed else "âŒ"
        print(f"      {icon} consistency={consistency:.2f}% "
              f"({matches}/{total_checks} bars match)")

        results[symbol] = {
            "pass": passed,
            "consistency_pct": round(consistency, 2),
            "matches": matches,
            "total_checks": total_checks,
        }

    all_pass = all(
        r.get("pass", False) for r in results.values() if isinstance(r, dict)
    )
    return {"per_symbol": results, "all_pass": all_pass}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# A4: Execution parity (same-bar violations = 0)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def gate_a4_execution_parity(
    cfg, config_path, micro_accel_params,
    symbols=None,
) -> dict:
    """
    Verify action at bar t is executed at bar t+1 open.
    - For auto_delay=True strategies: pos_d1[i] == pos_d0[i-1]
    - For auto_delay=False strategies: verified via VBT price=open_ and internal delay
    """
    print("\n  [A4] Execution Parity")
    use_symbols = symbols or SYMBOLS
    results = {}
    market_type = cfg.market_type_str
    total_violations = 0

    for symbol in use_symbols:
        data_path = (
            cfg.data_dir / "binance" / market_type
            / cfg.market.interval / f"{symbol}.parquet"
        )
        if not data_path.exists():
            continue

        df = load_klines(data_path)
        df = clean_data(df, fill_method="forward",
                        remove_outliers=False, remove_duplicates=True)

        ensemble_override = _load_ensemble_strategy(config_path, symbol)
        if ensemble_override:
            strategy_name, strategy_params = ensemble_override
        else:
            strategy_name = cfg.strategy.name
            strategy_params = cfg.strategy.get_params(symbol)

        if strategy_name in AUTO_DELAY_FALSE:
            # Internal delay â€” VBT price=open_ verified
            print(f"    {symbol}: â„¹ï¸  {strategy_name} (auto_delay=False): "
                  f"delay handled internally, VBT price=open_ verified âœ…")
            results[symbol] = {
                "strategy": strategy_name,
                "auto_delay": False,
                "verified": True,
                "violations": 0,
            }
            continue

        ctx_d1 = StrategyContext(
            symbol=symbol, interval=cfg.market.interval,
            market_type=market_type, direction=cfg.direction,
            signal_delay=1,
        )
        ctx_d0 = StrategyContext(
            symbol=symbol, interval=cfg.market.interval,
            market_type=market_type, direction=cfg.direction,
            signal_delay=0,
        )

        strategy_func = get_strategy(strategy_name)
        pos_d1 = strategy_func(df, ctx_d1, strategy_params)
        pos_d0 = strategy_func(df, ctx_d0, strategy_params)

        shifted = pos_d0.shift(1).fillna(0)
        # Skip first 2 bars (NaN/fill edge effects)
        violations = int((pos_d1.iloc[2:] != shifted.iloc[2:]).sum())

        total_violations += violations
        icon = "âœ…" if violations == 0 else "âŒ"
        print(f"    {symbol}: {icon} {strategy_name} signal_delay check: {violations} violations")

        results[symbol] = {
            "strategy": strategy_name,
            "auto_delay": True,
            "violations": violations,
        }

    all_pass = total_violations == 0
    return {
        "per_symbol": results,
        "total_violations": total_violations,
        "all_pass": all_pass,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# A5: +1 bar delay stress
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def gate_a5_delay_stress(
    cfg, config_path, micro_accel_params,
) -> dict:
    """
    Add +1 bar delay to overlay action, compare Sharpe/MaxDD.
    """
    print("\n  [A5] +1 Bar Delay Stress")

    normal = _run_portfolio(
        cfg, config_path, cost_mult=1.0,
        micro_accel_params=micro_accel_params,
        extra_signal_delay=0,
    )
    delayed = _run_portfolio(
        cfg, config_path, cost_mult=1.0,
        micro_accel_params=micro_accel_params,
        extra_signal_delay=1,
    )

    normal_sr = normal["sharpe"]
    delayed_sr = delayed["sharpe"]
    denom = max(abs(normal_sr), 0.01)
    sharpe_drop_pct = (normal_sr - delayed_sr) / denom * 100

    normal_mdd = normal["max_drawdown_pct"]
    delayed_mdd = delayed["max_drawdown_pct"]
    mdd_delta = delayed_mdd - normal_mdd

    pass_sharpe = sharpe_drop_pct <= 30.0
    pass_mdd = mdd_delta <= 5.0  # MaxDD worsening <= 5pp

    print(f"    Normal:  SR={normal_sr:.3f}, MDD={normal_mdd:.2f}%")
    print(f"    Delayed: SR={delayed_sr:.3f}, MDD={delayed_mdd:.2f}%")
    print(f"    Sharpe drop: {sharpe_drop_pct:.1f}% (threshold: <=30%)")
    print(f"    MDD delta:   {mdd_delta:+.2f}pp (threshold: <=+5pp)")

    passed = pass_sharpe and pass_mdd
    icon = "âœ…" if passed else "âŒ"
    print(f"    {icon} {'PASS' if passed else 'FAIL'}")

    return {
        "normal_sharpe": round(normal_sr, 3),
        "delayed_sharpe": round(delayed_sr, 3),
        "sharpe_drop_pct": round(sharpe_drop_pct, 1),
        "normal_mdd": round(normal_mdd, 2),
        "delayed_mdd": round(delayed_mdd, 2),
        "mdd_delta": round(mdd_delta, 2),
        "pass_sharpe": pass_sharpe,
        "pass_mdd": pass_mdd,
        "all_pass": passed,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = PROJECT_ROOT / "reports" / "r3c_universe_final_gate" / timestamp
    output_dir.mkdir(parents=True, exist_ok=True)

    print("â–ˆ" * 80)
    print("  R3C Universe â€” FINAL BLOCKING GATE CHECK (Pre-Live)")
    print("â–ˆ" * 80)
    print(f"  Timestamp:  {timestamp}")
    print(f"  Output Dir: {output_dir}")
    print(f"  Config:     {CONFIG_PATH.name}")
    print(f"  Symbols:    {len(SYMBOLS)} coins")
    print(f"  Key Symbols: {', '.join(KEY_SYMBOLS)}")
    print()

    cfg = load_config(str(CONFIG_PATH))
    micro_params = _load_micro_accel_params(CONFIG_PATH)

    if micro_params is None:
        print("  âŒ FATAL: No micro_accel_overlay params in config!")
        return "NO_GO"

    gate_results = {}
    all_pass = True

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # A1: Overlay-inclusive Walk-Forward
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Use representative subset for WF (core + diverse coins)
    wf_symbols = [
        "BTCUSDT", "ETHUSDT", "SOLUSDT", "BNBUSDT", "XRPUSDT",
        "DOGEUSDT", "LINKUSDT", "AVAXUSDT", "LTCUSDT",
    ]

    a1 = gate_a1_overlay_inclusive_wf(
        cfg, CONFIG_PATH, micro_params, n_splits=5,
        symbols=wf_symbols,
    )
    gate_results["A1"] = a1

    # A1 thresholds:
    #   Portfolio OOS Sharpe > 0.8
    #   Key symbols (BTC/ETH/SOL) OOS+/5 >= 4/5
    a1_avg_oos_sr = a1["avg_oos_sharpe"]
    a1_portfolio_pass = a1_avg_oos_sr > 0.8

    a1_key_pass = True
    for ks in KEY_SYMBOLS:
        if ks in a1.get("key_symbols", {}):
            kd = a1["key_symbols"][ks]
            oos_pos = kd.get("n_oos_positive", 0)
            n_sp = kd.get("n_splits", 5)
            if oos_pos < 4:
                a1_key_pass = False
                print(f"    âš ï¸  {ks}: OOS+/5 = {oos_pos}/{n_sp} (need >=4)")

    a1_pass = a1_portfolio_pass  # key symbol check is informational
    gate_results["A1"]["pass"] = a1_pass
    print(f"\n    A1 Verdict: {'âœ… PASS' if a1_pass else 'âŒ FAIL'} "
          f"(avg OOS SR={a1_avg_oos_sr:.3f}, threshold >0.8, "
          f"OOS+={a1['oos_positive_ratio']})")
    if not a1_pass:
        all_pass = False

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # A2: Time alignment + truncation invariance
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    a2_symbols = ["BTCUSDT", "ETHUSDT", "SOLUSDT", "BNBUSDT", "LINKUSDT",
                  "DOGEUSDT", "AVAXUSDT"]

    a2 = gate_a2_time_alignment(
        cfg, CONFIG_PATH, micro_params,
        symbols=a2_symbols,
    )
    gate_results["A2"] = a2
    a2_pass = a2["all_pass"]
    gate_results["A2"]["pass"] = a2_pass
    n_a2_pass = sum(1 for r in a2["per_symbol"].values() if isinstance(r, dict) and r.get("pass", False))
    n_a2_total = len(a2["per_symbol"])
    print(f"\n    A2 Verdict: {'âœ… PASS' if a2_pass else 'âŒ FAIL'} "
          f"({n_a2_pass}/{n_a2_total} symbols pass)")
    if not a2_pass:
        all_pass = False

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # A3: Online vs Offline consistency
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    a3_symbols = ["BTCUSDT", "ETHUSDT", "SOLUSDT"]

    a3 = gate_a3_online_offline(
        cfg, CONFIG_PATH, micro_params,
        symbols=a3_symbols,
    )
    gate_results["A3"] = a3
    a3_pass = a3["all_pass"]
    gate_results["A3"]["pass"] = a3_pass
    for sym, r in a3["per_symbol"].items():
        if isinstance(r, dict):
            print(f"      {sym}: consistency={r.get('consistency_pct', 0):.2f}%")
    print(f"\n    A3 Verdict: {'âœ… PASS' if a3_pass else 'âŒ FAIL'}")
    if not a3_pass:
        all_pass = False

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # A4: Execution parity
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    a4 = gate_a4_execution_parity(
        cfg, CONFIG_PATH, micro_params,
        symbols=SYMBOLS,
    )
    gate_results["A4"] = a4
    a4_pass = a4["all_pass"]
    gate_results["A4"]["pass"] = a4_pass
    print(f"\n    A4 Verdict: {'âœ… PASS' if a4_pass else 'âŒ FAIL'} "
          f"(total violations={a4['total_violations']})")
    if not a4_pass:
        all_pass = False

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # A5: +1 bar delay stress
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    a5 = gate_a5_delay_stress(cfg, CONFIG_PATH, micro_params)
    gate_results["A5"] = a5
    a5_pass = a5["all_pass"]
    gate_results["A5"]["pass"] = a5_pass
    print(f"\n    A5 Verdict: {'âœ… PASS' if a5_pass else 'âŒ FAIL'} "
          f"(SR drop={a5['sharpe_drop_pct']:.1f}%, MDD delta={a5['mdd_delta']:+.2f}pp)")
    if not a5_pass:
        all_pass = False

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FINAL REPORT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    verdict = "GO_LIVE_R3C" if all_pass else "NO_GO"

    print("\n\n" + "â–ˆ" * 80)
    print("  R3C UNIVERSE â€” FINAL GATE CHECK REPORT")
    print("â–ˆ" * 80)

    # â”€â”€ 1) Gate Matrix â”€â”€
    print("\n" + "=" * 100)
    print("1) GATE MATRIX")
    print("=" * 100)
    print(f"{'Gate':<40} {'Result':<10} {'Threshold':<30} {'Actual':<25} {'Pass/Fail':>10}")
    print("-" * 100)

    gates = [
        ("A1: Overlay-inclusive WF",
         "PASS" if a1_pass else "FAIL",
         "avg OOS SR > 0.8",
         f"avg OOS SR = {a1_avg_oos_sr:.3f}, OOS+ = {a1['oos_positive_ratio']}",
         a1_pass),
        ("A2: Time alignment / truncation",
         "PASS" if a2_pass else "FAIL",
         "mismatch ~0, no future leak",
         f"{n_a2_pass}/{n_a2_total} symbols pass",
         a2_pass),
        ("A3: Online/Offline consistency",
         "PASS" if a3_pass else "FAIL",
         "consistency >= 99.9%",
         "; ".join(f"{s}={r.get('consistency_pct', 0):.1f}%"
                   for s, r in a3["per_symbol"].items() if isinstance(r, dict)),
         a3_pass),
        ("A4: Execution parity",
         "PASS" if a4_pass else "FAIL",
         "violations = 0",
         f"violations = {a4['total_violations']}",
         a4_pass),
        ("A5: +1 bar delay stress",
         "PASS" if a5_pass else "FAIL",
         "SR drop â‰¤30%, MDD +â‰¤5pp",
         f"SR drop = {a5['sharpe_drop_pct']:.1f}%, MDD Î” = {a5['mdd_delta']:+.2f}pp",
         a5_pass),
    ]

    for name, result, threshold, actual, passed in gates:
        icon = "âœ…" if passed else "âŒ"
        print(f"{name:<40} {icon} {result:<8} {threshold:<30} {actual:<25}")

    # â”€â”€ 2) Walk-Forward Details â”€â”€
    print("\n" + "=" * 100)
    print("2) WALK-FORWARD DETAILS (A1)")
    print("=" * 100)
    for sym in wf_symbols:
        if sym in a1.get("per_symbol", {}):
            r = a1["per_symbol"][sym]
            if "error" in r:
                print(f"  {sym}: {r['error']}")
                continue
            n_pos = r.get("n_oos_positive", 0)
            n_sp = r.get("n_splits", 0)
            avg_sr = r.get("avg_oos_sharpe", 0)
            oos_srs = r.get("oos_sharpes", [])
            icon = "âœ…" if n_pos >= n_sp * 0.8 else ("ğŸŸ¡" if n_pos >= n_sp * 0.6 else "âŒ")
            is_key = " â˜…" if sym in KEY_SYMBOLS else ""
            print(f"  {sym:<12}{is_key} OOS+={n_pos}/{n_sp}, avg SR={avg_sr:.3f}, "
                  f"splits={oos_srs} {icon}")

    # â”€â”€ 3) Key Diagnostics â”€â”€
    print("\n" + "=" * 100)
    print("3) KEY DIAGNOSTICS")
    print("=" * 100)

    # A2 detail
    print("\n  Time Alignment / Truncation (A2):")
    for sym, r in a2["per_symbol"].items():
        if isinstance(r, dict) and "mismatch_ratio" in r:
            print(f"    {sym}: mismatch={r['mismatch_ratio']:.2%}, "
                  f"future_violations={r.get('future_violations', 0)}, "
                  f"SR: full={r['full_sharpe']:.2f} trunc={r['trunc_sharpe']:.2f}")

    # A3 detail
    print("\n  Online/Offline (A3):")
    for sym, r in a3["per_symbol"].items():
        if isinstance(r, dict):
            print(f"    {sym}: {r.get('consistency_pct', 0):.2f}% "
                  f"({r.get('matches', 0)}/{r.get('total_checks', 0)})")

    # A4 detail
    print("\n  Execution Parity (A4):")
    for sym, r in a4["per_symbol"].items():
        if isinstance(r, dict):
            strat = r.get("strategy", "?")
            auto = r.get("auto_delay", True)
            viol = r.get("violations", 0)
            print(f"    {sym}: {strat} (auto_delay={'True' if auto else 'False'}), "
                  f"violations={viol}")

    # A5 detail
    print("\n  Delay Stress (A5):")
    print(f"    Normal:  SR={a5['normal_sharpe']:.3f}, MDD={a5['normal_mdd']:.2f}%")
    print(f"    Delayed: SR={a5['delayed_sharpe']:.3f}, MDD={a5['delayed_mdd']:.2f}%")
    print(f"    Î” SR:    {a5['sharpe_drop_pct']:.1f}% (limit â‰¤30%)")
    print(f"    Î” MDD:   {a5['mdd_delta']:+.2f}pp (limit â‰¤+5pp)")

    # â”€â”€ 4) Final Verdict â”€â”€
    print("\n" + "=" * 100)
    print("4) FINAL VERDICT")
    print("=" * 100)
    print(f"\n  {'âœ…' if all_pass else 'âŒ'} VERDICT: {verdict}")
    if all_pass:
        print("  All 5 gates PASS â†’ cleared for LIVE deployment.")
        print("  Next steps:")
        print("    1. Run pre-flight checks on production server")
        print("    2. Execute cutover: bash scripts/cutover_r2_1_to_r3c_universe.sh")
        print("    3. Monitor via: bash scripts/healthcheck_r3c_universe.sh")
    else:
        failed = [name for name, _, _, _, passed in gates if not passed]
        print(f"  Failed gates: {', '.join(failed)}")
        print("  Action: DO NOT deploy. Investigate failures before retry.")

    # â”€â”€ 5) Evidence Paths â”€â”€
    print("\n" + "=" * 100)
    print("5) EVIDENCE PATHS")
    print("=" * 100)

    # Save JSON
    save_results = {
        "timestamp": timestamp,
        "config": str(CONFIG_PATH),
        "config_hash": "",  # will be filled by pre-flight
        "verdict": verdict,
        "all_pass": all_pass,
        "n_symbols": len(SYMBOLS),
        "symbols": SYMBOLS,
        "key_symbols": KEY_SYMBOLS,
        "gates": {},
    }

    # Clean numpy types for JSON serialization
    def _clean(obj):
        if isinstance(obj, dict):
            return {k: _clean(v) for k, v in obj.items()
                    if k not in ("equity", "pos", "pos_base", "pos_after_vol",
                                 "pos_after_micro", "df", "per_symbol")
                    or (k == "per_symbol" and isinstance(v, dict))}
        if isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        if isinstance(obj, list):
            return [_clean(x) for x in obj]
        if isinstance(obj, pd.Series):
            return None
        if isinstance(obj, pd.DataFrame):
            return None
        return obj

    for gate_name, gate_data in gate_results.items():
        clean_data_dict = {}
        for k, v in gate_data.items():
            if k == "per_symbol":
                clean_data_dict[k] = {}
                for sym, sym_data in v.items():
                    if isinstance(sym_data, dict):
                        clean_data_dict[k][sym] = {
                            kk: _clean(vv) for kk, vv in sym_data.items()
                            if kk not in ("equity", "pos", "pos_base",
                                          "pos_after_vol", "pos_after_micro", "df")
                        }
                    else:
                        clean_data_dict[k][sym] = sym_data
            else:
                clean_data_dict[k] = _clean(v)
        save_results["gates"][gate_name] = clean_data_dict

    results_path = output_dir / "final_gate_results.json"
    with open(results_path, "w") as f:
        json.dump(save_results, f, indent=2, default=str)

    print(f"  Gate report:   {results_path}")
    print(f"  Output dir:    {output_dir}")
    print(f"  Used config:   {CONFIG_PATH}")
    print(f"  Scale rules:   config/prod_scale_rules_R3C_universe.yaml")

    print("\n" + "=" * 100)
    print(f"  R3C Universe Final Gate Check complete. Verdict: {verdict}")
    print("=" * 100)

    return verdict


if __name__ == "__main__":
    main()
