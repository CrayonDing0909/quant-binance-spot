"""
è¡ç”Ÿå“æ•¸æ“šä¸‹è¼‰å·¥å…· (Phase 0A)

å¾ Binance Futures API ä¸‹è¼‰ä»¥ä¸‹è¡ç”Ÿå“æ•¸æ“šï¼š
    1. Long/Short Ratio (Account-level)
    2. Top Trader Long/Short Ratio (Account + Position)
    3. Taker Buy/Sell Volume Ratio
    4. CVD (Cumulative Volume Delta) â€” å¾ Taker Volume è¡ç”Ÿ

æ•¸æ“šä¾†æºï¼š
    A. Binance Vision (data.binance.vision) â€” å®Œæ•´æ­·å² (2021-12 è‡³ä»Š, 5m)
       æ¯æ—¥ metrics CSV å·²åŒ…å« LSR, Taker Vol Ratio ç­‰
    B. Binance Futures API â€” å³æ™‚ä½†åƒ… ~30 å¤©æ­·å² (500 records)

å„²å­˜è·¯å¾‘ï¼š
    data/binance/futures/derivatives/{metric}/{SYMBOL}.parquet

ä½¿ç”¨ç¯„ä¾‹ï¼š
    # å¾ Binance Vision ä¸‹è¼‰å…¨éƒ¨è¡ç”Ÿå“æ•¸æ“šï¼ˆæ¨è–¦ï¼Œå®Œæ•´æ­·å²ï¼‰
    PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT ETHUSDT

    # å¾ Binance API ä¸‹è¼‰æœ€è¿‘ 30 å¤©ï¼ˆå³æ™‚ï¼Œç”¨æ–¼è£œé½Šæœ€æ–°æ•¸æ“šï¼‰
    PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT --source api

    # åªä¸‹è¼‰ç‰¹å®šæŒ‡æ¨™
    PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT --metrics lsr taker_vol

    # æŸ¥çœ‹å·²ä¸‹è¼‰æ•¸æ“šçš„è¦†è“‹ç‡å ±å‘Š
    PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT --coverage
"""
from __future__ import annotations

import argparse
import io
import logging
import time
import zipfile
from datetime import datetime, timezone
from pathlib import Path

import numpy as np
import pandas as pd

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å¸¸æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DATA_DIR = Path("data/binance/futures/derivatives")

# Binance Vision metrics CSV æ¬„ä½æ˜ å°„
# CSV columns: create_time, symbol, sum_open_interest, sum_open_interest_value,
#   count_toptrader_long_short_ratio, sum_toptrader_long_short_ratio,
#   count_long_short_ratio, sum_taker_long_short_vol_ratio
VISION_METRIC_MAP = {
    "lsr": {
        "raw_col": "count_long_short_ratio",
        "description": "Long/Short Account Ratio (å…¨å¸³æˆ¶)",
    },
    "top_lsr_account": {
        "raw_col": "count_toptrader_long_short_ratio",
        "description": "Top Trader Long/Short Ratio (å¸³æˆ¶æ•¸)",
    },
    "top_lsr_position": {
        "raw_col": "sum_toptrader_long_short_ratio",
        "description": "Top Trader Long/Short Ratio (æŒå€‰é‡)",
    },
    "taker_vol_ratio": {
        "raw_col": "sum_taker_long_short_vol_ratio",
        "description": "Taker Buy/Sell Volume Ratio",
    },
}

# Binance API endpoint æ˜ å°„
API_ENDPOINTS = {
    "lsr": "/futures/data/globalLongShortAccountRatio",
    "top_lsr_account": "/futures/data/topLongShortAccountRatio",
    "top_lsr_position": "/futures/data/topLongShortPositionRatio",
    "taker_vol_ratio": "/futures/data/takerlongshortRatio",
}

ALL_METRICS = list(VISION_METRIC_MAP.keys())

VISION_BASE_URL = "https://data.binance.vision/data/futures/um/daily/metrics"
VISION_EARLIEST_DATE = "2021-12-01"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Binance Vision ä¸‹è¼‰ï¼ˆå®Œæ•´æ­·å²ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_vision_metrics(
    symbol: str,
    start: str | None = None,
    end: str | None = None,
    interval: str = "1h",
    cache_dir: Path | None = None,
) -> dict[str, pd.Series]:
    """
    å¾ data.binance.vision ä¸‹è¼‰æ¯æ—¥ metrics CSVï¼Œæå–æ‰€æœ‰è¡ç”Ÿå“æŒ‡æ¨™

    Returns:
        dict[metric_name, pd.Series] â€” æ¯å€‹æŒ‡æ¨™ä¸€å€‹ Series
    """
    import requests

    start_date = pd.Timestamp(start or VISION_EARLIEST_DATE, tz="UTC").normalize()
    end_date = (
        pd.Timestamp(end, tz="UTC").normalize()
        if end
        else pd.Timestamp.now(tz="UTC").normalize() - pd.Timedelta(days=2)
    )
    earliest = pd.Timestamp(VISION_EARLIEST_DATE, tz="UTC")
    if start_date < earliest:
        start_date = earliest

    if cache_dir is None:
        cache_dir = Path("data/binance/futures/open_interest/vision_cache") / symbol
    cache_dir.mkdir(parents=True, exist_ok=True)

    dates = pd.date_range(start=start_date, end=end_date, freq="D")
    all_dfs: list[pd.DataFrame] = []
    n_cached = n_downloaded = n_failed = 0

    logger.info(
        f"ğŸ“¥ Vision metrics: {symbol} {start_date:%Y-%m-%d} â†’ {end_date:%Y-%m-%d} "
        f"({len(dates)} days)"
    )

    for dt in dates:
        date_str = dt.strftime("%Y-%m-%d")
        csv_cache = cache_dir / f"{symbol}-metrics-{date_str}.csv"

        if csv_cache.exists():
            try:
                df_day = pd.read_csv(csv_cache)
                if not df_day.empty:
                    all_dfs.append(df_day)
                    n_cached += 1
                    continue
            except Exception:
                pass

        zip_url = f"{VISION_BASE_URL}/{symbol}/{symbol}-metrics-{date_str}.zip"
        try:
            resp = requests.get(zip_url, timeout=15)
            if resp.status_code == 404:
                continue
            resp.raise_for_status()

            with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:
                csv_name = zf.namelist()[0]
                with zf.open(csv_name) as f:
                    df_day = pd.read_csv(f)

            df_day.to_csv(csv_cache, index=False)
            all_dfs.append(df_day)
            n_downloaded += 1

        except Exception as e:
            n_failed += 1
            if n_failed <= 3:
                logger.debug(f"  skip {date_str}: {e}")

        total = n_cached + n_downloaded + n_failed
        if total > 0 and total % 100 == 0:
            logger.info(
                f"  ... {total}/{len(dates)} days "
                f"(cached={n_cached}, dl={n_downloaded}, fail={n_failed})"
            )

    if not all_dfs:
        logger.warning(f"âš ï¸  No vision metrics data for {symbol}")
        return {}

    raw = pd.concat(all_dfs, ignore_index=True)
    raw["create_time"] = pd.to_datetime(raw["create_time"], utc=True)
    raw = raw.sort_values("create_time")
    raw = raw.set_index("create_time")
    raw = raw[~raw.index.duplicated(keep="last")]

    logger.info(
        f"âœ… Vision raw: {symbol} {len(raw)} rows "
        f"(cached={n_cached}, dl={n_downloaded}, fail={n_failed})"
    )

    # æå–å„æŒ‡æ¨™ä¸¦ resample
    resample_map = {
        "5m": "5min", "15m": "15min", "30m": "30min",
        "1h": "1h", "2h": "2h", "4h": "4h", "1d": "1D",
    }
    freq = resample_map.get(interval, "1h")

    results: dict[str, pd.Series] = {}
    for metric_name, info in VISION_METRIC_MAP.items():
        col = info["raw_col"]
        if col not in raw.columns:
            logger.warning(f"  {metric_name}: column '{col}' not found, skipping")
            continue

        series = pd.to_numeric(raw[col], errors="coerce")
        if freq != "5min":
            series = series.resample(freq).last().dropna()
        else:
            series = series.dropna()

        series.name = metric_name
        results[metric_name] = series

        if not series.empty:
            logger.info(
                f"  {metric_name}: {len(series)} bars @ {interval} "
                f"({series.index[0]:%Y-%m-%d} â†’ {series.index[-1]:%Y-%m-%d})"
            )

    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Binance API ä¸‹è¼‰ï¼ˆæœ€è¿‘ 30 å¤©ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_api_metric(
    symbol: str,
    metric: str,
    interval: str = "1h",
    limit: int = 500,
) -> pd.Series:
    """
    å¾ Binance Futures API ä¸‹è¼‰å–®ä¸€è¡ç”Ÿå“æŒ‡æ¨™ï¼ˆæœ€è¿‘ ~30 å¤©ï¼‰

    Returns:
        pd.Series indexed by UTC timestamp
    """
    from qtrade.data.binance_futures_client import BinanceFuturesHTTP

    endpoint = API_ENDPOINTS.get(metric)
    if endpoint is None:
        raise ValueError(f"Unknown metric: {metric}. Available: {list(API_ENDPOINTS.keys())}")

    # Binance API ä½¿ç”¨ 'period' åƒæ•¸
    period_map = {
        "5m": "5m", "15m": "15m", "30m": "30m",
        "1h": "1h", "2h": "2h", "4h": "4h",
        "6h": "6h", "8h": "8h", "12h": "12h", "1d": "1d",
    }
    period = period_map.get(interval, "1h")

    client = BinanceFuturesHTTP()
    try:
        records = client.get(endpoint, {
            "symbol": symbol,
            "period": period,
            "limit": min(limit, 500),
        })
    except Exception as e:
        logger.error(f"âŒ API fetch {metric} {symbol}: {e}")
        return pd.Series(dtype=float, name=metric)

    if not records:
        return pd.Series(dtype=float, name=metric)

    df = pd.DataFrame(records)

    # API å›å‚³æ ¼å¼ï¼štimestamp, symbol, longShortRatio / buySellRatio ç­‰
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)

    # æ ¹æ“šä¸åŒ endpoint å–å¾—å€¼æ¬„ä½
    value_col = None
    for col_candidate in ["longShortRatio", "buySellRatio", "longAccount", "longPosition"]:
        if col_candidate in df.columns:
            value_col = col_candidate
            break

    if value_col is None:
        # fallback: å–ç¬¬ä¸€å€‹æ•¸å€¼æ¬„
        num_cols = [c for c in df.columns if c not in ("timestamp", "symbol")]
        value_col = num_cols[0] if num_cols else None

    if value_col is None:
        logger.warning(f"  {metric} {symbol}: no value column found")
        return pd.Series(dtype=float, name=metric)

    series = pd.to_numeric(df.set_index("timestamp")[value_col], errors="coerce")
    series = series.sort_index()
    series = series[~series.index.duplicated(keep="last")]
    series.name = metric

    if not series.empty:
        logger.info(
            f"âœ… API {metric} {symbol}: {len(series)} records "
            f"({series.index[0]:%Y-%m-%d} â†’ {series.index[-1]:%Y-%m-%d})"
        )

    return series


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CVD (Cumulative Volume Delta) è¨ˆç®—
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compute_cvd(taker_vol_ratio: pd.Series) -> pd.Series:
    """
    å¾ Taker Buy/Sell Volume Ratio è¿‘ä¼¼è¨ˆç®— CVD

    å…¬å¼ï¼š
        taker_vol_ratio > 1 â†’ è²·æ–¹ä¸»å°ï¼ˆè²·å…¥é‡ > è³£å‡ºé‡ï¼‰
        delta = (ratio - 1) / (ratio + 1)  â†’ æ¨™æº–åŒ–åˆ° [-1, 1]
        CVD = cumsum(delta)

    é€™æ˜¯ä¸€å€‹è¿‘ä¼¼å€¼ â€” çœŸæ­£çš„ CVD éœ€è¦é€ç­†æˆäº¤æ•¸æ“šã€‚
    ä½† taker ratio çš„ç´¯ç©è®ŠåŒ–èƒ½æ•æ‰åŒæ¨£çš„è¶¨å‹¢è¨Šè™Ÿã€‚

    Args:
        taker_vol_ratio: Taker Buy/Sell Volume Ratio Series

    Returns:
        CVD ç´¯ç©åºåˆ—
    """
    if taker_vol_ratio.empty:
        return pd.Series(dtype=float, name="cvd")

    # æ¨™æº–åŒ– delta: ratio > 1 â†’ positive, ratio < 1 â†’ negative
    ratio = taker_vol_ratio.copy()
    delta = (ratio - 1.0) / (ratio + 1.0)
    delta = delta.fillna(0.0).clip(-1.0, 1.0)

    cvd = delta.cumsum()
    cvd.name = "cvd"
    return cvd


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å„²å­˜ / è¼‰å…¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_derivative(
    series: pd.Series,
    symbol: str,
    metric: str,
    data_dir: Path = DATA_DIR,
) -> Path:
    """å„²å­˜è¡ç”Ÿå“æŒ‡æ¨™åˆ° parquet"""
    path = data_dir / metric / f"{symbol}.parquet"
    path.parent.mkdir(parents=True, exist_ok=True)
    df = series.to_frame(name=metric)
    df.to_parquet(path, index=True)
    logger.info(f"ğŸ’¾ Saved {metric}/{symbol}: {len(df)} rows â†’ {path}")
    return path


def load_derivative(
    symbol: str,
    metric: str,
    data_dir: Path = DATA_DIR,
) -> pd.Series | None:
    """è¼‰å…¥è¡ç”Ÿå“æŒ‡æ¨™"""
    path = data_dir / metric / f"{symbol}.parquet"
    if not path.exists():
        return None
    try:
        df = pd.read_parquet(path)
        if metric in df.columns:
            return df[metric]
        return df.iloc[:, 0]
    except Exception as e:
        logger.warning(f"âš ï¸  Load {metric}/{symbol} failed: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  è¦†è“‹ç‡å ±å‘Š
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def coverage_report(
    symbols: list[str],
    metrics: list[str] | None = None,
    data_dir: Path = DATA_DIR,
) -> pd.DataFrame:
    """ç”Ÿæˆè¡ç”Ÿå“æ•¸æ“šè¦†è“‹ç‡å ±å‘Š"""
    if metrics is None:
        metrics = ALL_METRICS + ["cvd"]

    rows = []
    for symbol in symbols:
        for metric in metrics:
            series = load_derivative(symbol, metric, data_dir)
            if series is None or series.empty:
                rows.append({
                    "symbol": symbol,
                    "metric": metric,
                    "rows": 0,
                    "start": None,
                    "end": None,
                    "coverage_days": 0,
                })
            else:
                days = (series.index[-1] - series.index[0]).days
                rows.append({
                    "symbol": symbol,
                    "metric": metric,
                    "rows": len(series),
                    "start": series.index[0].strftime("%Y-%m-%d"),
                    "end": series.index[-1].strftime("%Y-%m-%d"),
                    "coverage_days": days,
                })

    return pd.DataFrame(rows)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ä¸»ç¨‹å¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="Binance è¡ç”Ÿå“æ•¸æ“šä¸‹è¼‰å·¥å…·ï¼ˆLSR, Taker Vol, CVDï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # ä¸‹è¼‰å…¨éƒ¨æŒ‡æ¨™ï¼ˆå¾ Binance Visionï¼Œå®Œæ•´æ­·å²ï¼‰
  PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT ETHUSDT

  # å¾ Binance API ä¸‹è¼‰æœ€è¿‘ 30 å¤©
  PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT --source api

  # åªä¸‹è¼‰ç‰¹å®šæŒ‡æ¨™
  PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT --metrics lsr taker_vol_ratio

  # æŸ¥çœ‹è¦†è“‹ç‡å ±å‘Š
  PYTHONPATH=src python scripts/fetch_derivatives_data.py --symbols BTCUSDT ETHUSDT --coverage
        """,
    )
    parser.add_argument(
        "--symbols", nargs="+", required=True,
        help="äº¤æ˜“å°åˆ—è¡¨ (e.g. BTCUSDT ETHUSDT)",
    )
    parser.add_argument(
        "--metrics", nargs="+", default=None,
        choices=ALL_METRICS,
        help=f"è¦ä¸‹è¼‰çš„æŒ‡æ¨™ï¼ˆé è¨­å…¨éƒ¨: {ALL_METRICS}ï¼‰",
    )
    parser.add_argument(
        "--source", default="vision",
        choices=["vision", "api", "both"],
        help="æ•¸æ“šä¾†æº: vision=å®Œæ•´æ­·å², api=æœ€è¿‘30å¤©, both=åˆä½µ",
    )
    parser.add_argument(
        "--interval", default="1h",
        help="K ç·šé€±æœŸ (é è¨­: 1h)",
    )
    parser.add_argument(
        "--start", default=None,
        help="é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--end", default=None,
        help="çµæŸæ—¥æœŸ (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--coverage", action="store_true",
        help="åªé¡¯ç¤ºè¦†è“‹ç‡å ±å‘Šï¼Œä¸ä¸‹è¼‰",
    )
    parser.add_argument(
        "--data-dir", default=str(DATA_DIR),
        help=f"æ•¸æ“šå„²å­˜ç›®éŒ„ (é è¨­: {DATA_DIR})",
    )

    args = parser.parse_args()
    data_dir = Path(args.data_dir)
    metrics = args.metrics or ALL_METRICS

    # è¦†è“‹ç‡å ±å‘Šæ¨¡å¼
    if args.coverage:
        report = coverage_report(args.symbols, data_dir=data_dir)
        if report.empty:
            print("âŒ ç„¡å·²ä¸‹è¼‰çš„æ•¸æ“š")
            return
        print("\nğŸ“Š è¡ç”Ÿå“æ•¸æ“šè¦†è“‹ç‡å ±å‘Š")
        print("=" * 80)
        for symbol in args.symbols:
            sym_data = report[report["symbol"] == symbol]
            print(f"\n  {symbol}:")
            for _, row in sym_data.iterrows():
                if row["rows"] == 0:
                    print(f"    {row['metric']:<20} âŒ ç„¡æ•¸æ“š")
                else:
                    print(
                        f"    {row['metric']:<20} âœ… {row['rows']:>6} rows  "
                        f"{row['start']} â†’ {row['end']}  ({row['coverage_days']}d)"
                    )
        print()
        return

    # ä¸‹è¼‰æ¨¡å¼
    for symbol in args.symbols:
        print(f"\n{'='*60}")
        print(f"  ğŸ“¥ {symbol}")
        print(f"{'='*60}")

        all_series: dict[str, pd.Series] = {}

        # 1. Vision ä¾†æº
        if args.source in ("vision", "both"):
            vision_data = fetch_vision_metrics(
                symbol, start=args.start, end=args.end, interval=args.interval,
            )
            for m in metrics:
                if m in vision_data:
                    all_series[m] = vision_data[m]

        # 2. API ä¾†æº
        if args.source in ("api", "both"):
            for m in metrics:
                api_series = fetch_api_metric(
                    symbol, m, interval=args.interval,
                )
                if api_series.empty:
                    continue

                if m in all_series and args.source == "both":
                    # åˆä½µï¼švision ç‚ºä¸»ï¼Œapi è£œé½Šå°¾éƒ¨
                    combined = pd.concat([all_series[m], api_series])
                    combined = combined[~combined.index.duplicated(keep="last")]
                    combined = combined.sort_index()
                    all_series[m] = combined
                    logger.info(f"  {m}: merged vision + api â†’ {len(combined)} rows")
                else:
                    all_series[m] = api_series

            # API æ¨¡å¼ä¹‹é–“åš rate limiting
            time.sleep(0.5)

        # 3. å„²å­˜å„æŒ‡æ¨™
        for m, series in all_series.items():
            save_derivative(series, symbol, m, data_dir)

        # 4. è¨ˆç®—ä¸¦å„²å­˜ CVD
        if "taker_vol_ratio" in all_series:
            cvd = compute_cvd(all_series["taker_vol_ratio"])
            save_derivative(cvd, symbol, "cvd", data_dir)
            logger.info(f"  CVD: {len(cvd)} bars computed from taker_vol_ratio")

    print(f"\nâœ… ä¸‹è¼‰å®Œæˆï¼æ•¸æ“šç›®éŒ„: {data_dir}")


if __name__ == "__main__":
    main()
