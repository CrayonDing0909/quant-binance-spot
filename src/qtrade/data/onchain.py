"""
éˆä¸Šæ•¸æ“šæ¨¡çµ„

æ”¯æ´ï¼š
    1. DeFi Llama â€” TVLã€Stablecoin æµå‹•æ€§ï¼ˆå…è²»ï¼Œç„¡éœ€ API keyï¼‰
    2. CryptoQuant (free tier) â€” Exchange Reserveï¼ˆéœ€ API keyï¼‰
    3. Glassnode (free tier) â€” BTC/ETH åŸºç¤Žéˆä¸ŠæŒ‡æ¨™ï¼ˆéœ€ API keyï¼‰

é€™äº›æ•¸æ“šä¸»è¦ä½œç‚º Regime Indicatorï¼ˆé¢¨éšªåå¥½ã€å®è§€ç’°å¢ƒï¼‰ï¼Œ
ä¸é©åˆé«˜é »ä¿¡è™Ÿï¼ˆå»¶é² 1-10 åˆ†é˜ ~ æ•¸å°æ™‚ï¼‰ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    from qtrade.data.onchain import (
        load_onchain,
        save_onchain,
        align_onchain_to_klines,
        compute_onchain_coverage,
    )
"""
from __future__ import annotations

import logging
from pathlib import Path
from typing import Optional

import pandas as pd

logger = logging.getLogger(__name__)

_BASE_DIR = Path("data/onchain")


def save_onchain(
    data: pd.Series | pd.DataFrame,
    provider: str,
    metric: str,
    data_dir: Path = _BASE_DIR,
) -> Path:
    """å„²å­˜éˆä¸Šæ•¸æ“šåˆ° parquet"""
    path = data_dir / provider / f"{metric}.parquet"
    path.parent.mkdir(parents=True, exist_ok=True)
    if isinstance(data, pd.Series):
        data = data.to_frame()
    data.to_parquet(path, index=True)
    logger.info(f"ðŸ’¾ Saved {provider}/{metric}: {len(data)} rows â†’ {path}")
    return path


def load_onchain(
    provider: str,
    metric: str,
    data_dir: Path = _BASE_DIR,
) -> Optional[pd.DataFrame]:
    """è¼‰å…¥éˆä¸Šæ•¸æ“š"""
    path = data_dir / provider / f"{metric}.parquet"
    if not path.exists():
        return None
    try:
        df = pd.read_parquet(path)
        if not df.empty:
            logger.debug(f"ðŸ“‚ On-chain loaded: {provider}/{metric} ({len(df)} rows)")
        return df
    except Exception as e:
        logger.warning(f"âš ï¸  On-chain load failed ({provider}/{metric}): {e}")
        return None


def get_onchain_path(data_dir: Path, provider: str, metric: str) -> Path:
    """å–å¾—éˆä¸Šæ•¸æ“šæ¨™æº–è·¯å¾‘"""
    return data_dir / provider / f"{metric}.parquet"


def align_onchain_to_klines(
    onchain_data: pd.Series | pd.DataFrame | None,
    kline_index: pd.DatetimeIndex,
    max_ffill_bars: int = 24,
) -> pd.Series | pd.DataFrame | None:
    """
    å°‡éˆä¸Šæ•¸æ“šå°é½Šåˆ° K ç·šæ™‚é–“è»¸

    éˆä¸Šæ•¸æ“šé€šå¸¸æ˜¯ daily é »çŽ‡ï¼Œæ‰€ä»¥ max_ffill_bars é è¨­è¼ƒé«˜ï¼ˆ24 bars = 1d for 1h klinesï¼‰ã€‚
    åš´æ ¼å› æžœï¼šåªä½¿ç”¨ forward-fillï¼Œä¸ä½¿ç”¨æœªä¾†è³‡è¨Šã€‚

    Args:
        onchain_data: éˆä¸Šæ•¸æ“šï¼ˆSeries æˆ– DataFrameï¼‰
        kline_index: K ç·šæ™‚é–“ç´¢å¼•
        max_ffill_bars: æœ€å¤§ forward-fill barsï¼ˆdaily æ•¸æ“šå°é½Šåˆ° 1h æ™‚å»ºè­° 24ï¼‰

    Returns:
        å°é½Šå¾Œçš„ Series / DataFrame
    """
    if onchain_data is None:
        return None

    if isinstance(onchain_data, pd.DataFrame) and onchain_data.empty:
        return None
    if isinstance(onchain_data, pd.Series) and onchain_data.empty:
        return None

    data = onchain_data.copy()

    # Timezone alignment
    if kline_index.tz is None and data.index.tz is not None:
        data.index = data.index.tz_localize(None)
    elif kline_index.tz is not None and data.index.tz is None:
        data.index = data.index.tz_localize(kline_index.tz)

    aligned = data.reindex(kline_index, method="ffill", limit=max_ffill_bars)

    if isinstance(aligned, pd.Series):
        n_missing = aligned.isna().sum()
        n_total = len(aligned)
    else:
        n_missing = aligned.isna().all(axis=1).sum()
        n_total = len(aligned)

    if n_missing > 0:
        coverage = (n_total - n_missing) / n_total * 100
        logger.info(f"ðŸ“Š On-chain alignment: {n_total - n_missing}/{n_total} ({coverage:.1f}%)")

    return aligned


def compute_onchain_coverage(
    provider: str = "defillama",
    data_dir: Path = _BASE_DIR,
) -> pd.DataFrame:
    """è¨ˆç®—éˆä¸Šæ•¸æ“šè¦†è“‹çŽ‡"""
    provider_dir = data_dir / provider
    if not provider_dir.exists():
        return pd.DataFrame()

    rows = []
    for f in sorted(provider_dir.glob("*.parquet")):
        try:
            df = pd.read_parquet(f)
            metric = f.stem
            if df.empty:
                rows.append({"metric": metric, "rows": 0, "start": None, "end": None, "days": 0})
            else:
                days = (df.index[-1] - df.index[0]).days if hasattr(df.index, '__getitem__') else 0
                rows.append({
                    "metric": metric,
                    "rows": len(df),
                    "start": str(df.index[0])[:10] if len(df) > 0 else None,
                    "end": str(df.index[-1])[:10] if len(df) > 0 else None,
                    "days": days,
                })
        except Exception as e:
            rows.append({"metric": f.stem, "rows": 0, "start": None, "end": None, "days": 0})

    return pd.DataFrame(rows)
