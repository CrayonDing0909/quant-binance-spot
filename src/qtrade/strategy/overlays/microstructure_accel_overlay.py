"""
Microstructure Acceleration Overlay (R3 Track A)

ä¸æ”¹ R2.1 çš„ 1h æ–¹å‘åˆ¤æ–·ï¼Œåªåœ¨åŸ·è¡Œå±¤æ–°å¢ 5m/15mã€ŒåŠ é€Ÿ/æ¸›é€Ÿã€ï¼š
    - è¶¨å‹¢åŒå‘ä¸”å¾®çµæ§‹ç¢ºèªå¼· â†’ åŠ é€Ÿé€²å ´ / åŠ å€‰
    - è¶¨å‹¢åŒå‘ä½†å¾®çµæ§‹è½‰å¼± â†’ å»¶é²é€²å ´ / æ¸›å€‰
    - è¶¨å‹¢åå‘ä¸”å¾®çµæ§‹æ¥µç«¯ä¸åˆ© â†’ å¿«é€Ÿé™é¢¨éšª

å¾®çµæ§‹ç‰¹å¾µï¼ˆBinance-only å¯å¾—ï¼‰ï¼š
    1. Taker Buy/Sell Imbalanceï¼ˆ5m/15m OHLCV proxyï¼‰
    2. çŸ­çª— Realized Vol / Vol Regime
    3. åƒ¹æ ¼çŸ­çª—å‹•èƒ½æ–œç‡ï¼ˆEMA slope / return burstï¼‰
    4. OI change rateï¼ˆæ¬¡è¦ç‰¹å¾µï¼Œéœ€ OI è³‡æ–™ï¼‰

Anti-lookahead ä¿è­‰ï¼š
    - æ‰€æœ‰ç‰¹å¾µç”¨ [0, i] çš„è³‡æ–™è¨ˆç®—
    - 5m/15m ç‰¹å¾µ resample åˆ° 1h æ™‚ç”¨ lastï¼ˆbar i çµæŸæ™‚å¯å¾—ï¼‰
    - çµæœ position[i] çš„æ”¹å‹•åœ¨ bar[i+1] é–‹ç›¤åŸ·è¡Œ
    - èˆ‡ trade_on=next_open + signal_delay=1 ä¸€è‡´

Usage:
    from qtrade.strategy.overlays.microstructure_accel_overlay import (
        compute_micro_features,
        compute_accel_score,
        apply_accel_overlay,
    )
"""
from __future__ import annotations

import logging
from pathlib import Path

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  1. Taker Buy/Sell Imbalance Proxy
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _taker_imbalance_proxy(df: pd.DataFrame, window: int = 12) -> pd.Series:
    """
    å¾ OHLCV ä¼°ç®— Taker Buy/Sell Imbalance

    Proxy é‚è¼¯ï¼š
        close_position = (close - low) / (high - low)
        â†’ æ¥è¿‘ 1 è¡¨ç¤ºè²·æ–¹ä¸»å°ï¼Œæ¥è¿‘ 0 è¡¨ç¤ºè³£æ–¹ä¸»å°

    ç”¨ rolling mean å¹³æ»‘å¾Œå†æ¨™æº–åŒ–ç‚º [-1, 1]ï¼š
        imbalance = (rolling_mean(close_position) - 0.5) * 2

    Args:
        df: OHLCV DataFrameï¼ˆ5m æˆ– 15mï¼‰
        window: æ»¾å‹•çª—å£ï¼ˆbarsï¼‰

    Returns:
        Taker imbalance proxy [-1, 1]
    """
    hl_range = df["high"] - df["low"]
    # é¿å…é™¤ä»¥é›¶
    hl_range = hl_range.replace(0, np.nan)
    close_pos = (df["close"] - df["low"]) / hl_range
    close_pos = close_pos.fillna(0.5)  # flat bar â†’ neutral

    # Volume-weighted close position for better signal
    vol = df["volume"].replace(0, np.nan).fillna(1.0)
    vol_weighted = close_pos * vol
    vol_sum = vol.rolling(window, min_periods=max(window // 2, 1)).sum()
    weighted_mean = (
        vol_weighted.rolling(window, min_periods=max(window // 2, 1)).sum()
        / vol_sum
    )

    # Normalize to [-1, 1]
    imbalance = (weighted_mean - 0.5) * 2.0
    return imbalance.clip(-1.0, 1.0).fillna(0.0)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  2. Short-Window Realized Volatility
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _short_realized_vol(df: pd.DataFrame, window: int = 12) -> pd.Series:
    """
    çŸ­çª— Realized Volatilityï¼ˆå¹´åŒ–ï¼‰

    Args:
        df: OHLCV DataFrame
        window: æ»¾å‹•çª—å£ï¼ˆbarsï¼‰

    Returns:
        å¹´åŒ–æ³¢å‹•ç‡
    """
    returns = df["close"].pct_change()
    # Detect bar frequency for annualization
    if len(df) >= 2:
        freq_sec = (df.index[1] - df.index[0]).total_seconds()
        bars_per_year = 365.25 * 24 * 3600 / max(freq_sec, 1)
    else:
        bars_per_year = 8760  # default 1h

    rv = returns.rolling(window, min_periods=max(window // 2, 1)).std()
    rv_annualized = rv * np.sqrt(bars_per_year)
    return rv_annualized.fillna(0.0)


def _vol_regime_zscore(
    rv: pd.Series,
    long_window: int = 168,
) -> pd.Series:
    """
    Volatility regime z-score

    æ­£å€¼ = æ³¢å‹•é«˜æ–¼æ­·å²å¹³å‡ï¼ˆè¶¨å‹¢å¯èƒ½åŠ é€Ÿ or åè½‰ï¼‰
    è² å€¼ = æ³¢å‹•ä½æ–¼å¹³å‡ï¼ˆç›¤æ•´å¯èƒ½çµæŸï¼‰

    Args:
        rv: å·²è¨ˆç®—çš„ realized vol
        long_window: z-score è¨ˆç®—ç”¨é•·çª—å£

    Returns:
        vol z-score
    """
    rolling_mean = rv.rolling(long_window, min_periods=max(long_window // 4, 1)).mean()
    rolling_std = rv.rolling(long_window, min_periods=max(long_window // 4, 1)).std()
    rolling_std = rolling_std.replace(0, np.nan)
    z = (rv - rolling_mean) / rolling_std
    return z.fillna(0.0).clip(-4.0, 4.0)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  3. EMA Slope / Return Burst
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _ema_slope(df: pd.DataFrame, period: int = 12, norm_window: int = 48) -> pd.Series:
    """
    EMA æ–œç‡ï¼ˆæ¨™æº–åŒ–ç‚º z-scoreï¼‰

    è¡¡é‡åƒ¹æ ¼çŸ­æœŸå‹•èƒ½æ–¹å‘å’Œå¼·åº¦ã€‚
    æ­£å€¼ = ä¸Šæ¼²å‹•èƒ½ï¼Œè² å€¼ = ä¸‹è·Œå‹•èƒ½ã€‚

    Args:
        df: OHLCV DataFrame
        period: EMA é€±æœŸ
        norm_window: ç”¨æ–¼æ¨™æº–åŒ–æ–œç‡çš„çª—å£

    Returns:
        EMA slope z-score
    """
    ema = df["close"].ewm(span=period, adjust=False).mean()
    # Slope = bar-to-bar change of EMA, normalized by price
    slope = ema.diff() / df["close"].replace(0, np.nan)
    slope = slope.fillna(0.0)

    # Z-score normalization for comparability across assets
    roll_mean = slope.rolling(norm_window, min_periods=max(norm_window // 4, 1)).mean()
    roll_std = slope.rolling(norm_window, min_periods=max(norm_window // 4, 1)).std()
    roll_std = roll_std.replace(0, np.nan)
    z = (slope - roll_mean) / roll_std
    return z.fillna(0.0).clip(-4.0, 4.0)


def _return_burst(df: pd.DataFrame, window: int = 6) -> pd.Series:
    """
    Return Burst â€” çŸ­çª—ç´¯è¨ˆå ±é…¬ z-score

    æ•æ‰åƒ¹æ ¼çªç„¶åŠ é€Ÿæˆ–æ¸›é€Ÿçš„æ™‚åˆ»ã€‚

    Args:
        df: OHLCV DataFrame
        window: ç´¯è¨ˆå ±é…¬çª—å£ï¼ˆbarsï¼‰

    Returns:
        Return burst z-score
    """
    returns = df["close"].pct_change()
    cum_ret = returns.rolling(window, min_periods=max(window // 2, 1)).sum()

    # Z-score over longer window for context
    long_w = window * 8
    roll_mean = cum_ret.rolling(long_w, min_periods=max(long_w // 4, 1)).mean()
    roll_std = cum_ret.rolling(long_w, min_periods=max(long_w // 4, 1)).std()
    roll_std = roll_std.replace(0, np.nan)
    z = (cum_ret - roll_mean) / roll_std
    return z.fillna(0.0).clip(-4.0, 4.0)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  4. OI Change Rate
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _oi_change_rate(
    oi_series: pd.Series | None,
    lookback: int = 24,
    z_window: int = 168,
) -> pd.Series | None:
    """
    Open Interest è®ŠåŒ–ç‡ z-score

    æ­£å€¼ = OI å¿«é€Ÿå¢åŠ ï¼ˆæ–°éŒ¢é€²å ´ï¼‰
    è² å€¼ = OI å¿«é€Ÿæ¸›å°‘ï¼ˆå¹³å€‰é›¢å ´ï¼‰

    Args:
        oi_series: OI åºåˆ—ï¼ˆå·²å°é½Šåˆ° kline indexï¼‰
        lookback: OI è®ŠåŒ–ç‡å›çœ‹æœŸ
        z_window: z-score çª—å£

    Returns:
        OI change rate z-score, or None if no OI data
    """
    if oi_series is None or oi_series.empty:
        return None

    oi_change = oi_series.pct_change(lookback, fill_method=None)
    roll_mean = oi_change.rolling(z_window, min_periods=max(z_window // 4, 1)).mean()
    roll_std = oi_change.rolling(z_window, min_periods=max(z_window // 4, 1)).std()
    roll_std = roll_std.replace(0, np.nan)
    z = (oi_change - roll_mean) / roll_std
    return z.fillna(0.0).clip(-4.0, 4.0)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Resample Helper: Sub-hourly â†’ 1h
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _resample_to_1h(series: pd.Series, method: str = "last") -> pd.Series:
    """
    Resample sub-hourly series to 1h frequency.

    ä½¿ç”¨ label='left', closed='left' å°é½Šåˆ° 1h bar çš„ open_timeï¼Œ
    å–æœ€å¾Œä¸€å€‹å€¼ï¼ˆbar i çµæŸæ™‚å¯å¾—ï¼Œä¸å«æœªä¾†è³‡è¨Šï¼‰ã€‚

    Args:
        series: sub-hourly pd.Series with DatetimeIndex
        method: "last" | "mean" | "sum"

    Returns:
        1h frequency pd.Series
    """
    resampler = series.resample("1h", label="left", closed="left")
    if method == "last":
        return resampler.last()
    elif method == "mean":
        return resampler.mean()
    elif method == "sum":
        return resampler.sum()
    else:
        return resampler.last()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  compute_micro_features â€” å…¬é–‹ API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compute_micro_features(
    df_1h: pd.DataFrame,
    df_5m: pd.DataFrame | None = None,
    df_15m: pd.DataFrame | None = None,
    oi_series: pd.Series | None = None,
    params: dict | None = None,
) -> pd.DataFrame:
    """
    è¨ˆç®—æ‰€æœ‰å¾®çµæ§‹ç‰¹å¾µï¼Œå›å‚³å°é½Šåˆ° 1h index çš„ DataFrame

    ç‰¹å¾µæ¬„ä½ï¼š
        - taker_imbalance:  Taker Buy/Sell Imbalance proxy [-1, 1]
        - vol_regime_z:     Short-window vol regime z-score
        - ema_slope_z:      EMA slope z-score
        - return_burst_z:   Return burst z-score
        - oi_change_z:      OI change rate z-score (may be NaN)

    Anti-lookahead:
        - æ‰€æœ‰ sub-hourly ç‰¹å¾µ resample åˆ° 1h æ™‚ç”¨ "last"
          ï¼ˆ= è©²å°æ™‚æœ€å¾Œä¸€å€‹ sub-bar çš„å€¼ï¼Œbar i çµæŸæ™‚å·²çŸ¥ï¼‰
        - 1h ä¸Šçš„ç‰¹å¾µç›´æ¥è¨ˆç®—ï¼ˆèˆ‡åŸºç¤ç­–ç•¥åŒé »ç‡ï¼‰

    Args:
        df_1h: 1h OHLCV DataFrame
        df_5m: 5m OHLCV DataFrame (optional)
        df_15m: 15m OHLCV DataFrame (optional)
        oi_series: OI series aligned to 1h index (optional)
        params: feature computation parameters

    Returns:
        DataFrame aligned to df_1h.index with micro features
    """
    p = params or {}
    idx = df_1h.index

    # â”€â”€ Choose best sub-hourly frame for each feature â”€â”€
    # Priority: 5m > 15m > fallback to 1h
    micro_df = df_5m if df_5m is not None and len(df_5m) > 0 else (
        df_15m if df_15m is not None and len(df_15m) > 0 else None
    )

    features = pd.DataFrame(index=idx)

    # â”€â”€ Feature 1: Taker Imbalance â”€â”€
    taker_window = int(p.get("taker_window", 12))
    if micro_df is not None:
        raw_imb = _taker_imbalance_proxy(micro_df, window=taker_window)
        features["taker_imbalance"] = _resample_to_1h(raw_imb, "last").reindex(idx).ffill().fillna(0.0)
    else:
        # Fallback: compute from 1h directly (weaker signal)
        features["taker_imbalance"] = _taker_imbalance_proxy(df_1h, window=max(taker_window // 3, 3))

    # â”€â”€ Feature 2: Vol Regime Z-score â”€â”€
    vol_short_window = int(p.get("vol_short_window", 12))
    vol_long_window = int(p.get("vol_long_window", 168))
    if micro_df is not None:
        rv_micro = _short_realized_vol(micro_df, window=vol_short_window)
        vz_micro = _vol_regime_zscore(rv_micro, long_window=vol_long_window)
        features["vol_regime_z"] = _resample_to_1h(vz_micro, "last").reindex(idx).ffill().fillna(0.0)
    else:
        rv_1h = _short_realized_vol(df_1h, window=max(vol_short_window // 3, 3))
        features["vol_regime_z"] = _vol_regime_zscore(rv_1h, long_window=max(vol_long_window // 12, 14))

    # â”€â”€ Feature 3: EMA Slope â”€â”€
    ema_slope_period = int(p.get("ema_slope_period", 12))
    ema_slope_norm = int(p.get("ema_slope_norm_window", 48))
    if micro_df is not None:
        es_micro = _ema_slope(micro_df, period=ema_slope_period, norm_window=ema_slope_norm)
        features["ema_slope_z"] = _resample_to_1h(es_micro, "last").reindex(idx).ffill().fillna(0.0)
    else:
        features["ema_slope_z"] = _ema_slope(df_1h, period=max(ema_slope_period // 3, 3),
                                              norm_window=max(ema_slope_norm // 12, 6))

    # â”€â”€ Feature 4: Return Burst â”€â”€
    burst_window = int(p.get("return_burst_window", 6))
    if micro_df is not None:
        rb_micro = _return_burst(micro_df, window=burst_window)
        features["return_burst_z"] = _resample_to_1h(rb_micro, "last").reindex(idx).ffill().fillna(0.0)
    else:
        features["return_burst_z"] = _return_burst(df_1h, window=max(burst_window // 3, 2))

    # â”€â”€ Feature 5: OI Change Rate â”€â”€
    oi_lookback = int(p.get("oi_lookback", 24))
    oi_z_window = int(p.get("oi_z_window", 168))
    oi_z = _oi_change_rate(oi_series, lookback=oi_lookback, z_window=oi_z_window)
    if oi_z is not None:
        features["oi_change_z"] = oi_z.reindex(idx).ffill().fillna(0.0)
    else:
        features["oi_change_z"] = 0.0

    logger.info(
        f"ğŸ“Š Micro features computed: {list(features.columns)}, "
        f"source={'5m' if df_5m is not None else '15m' if df_15m is not None else '1h'}, "
        f"shape={features.shape}"
    )

    return features


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  compute_accel_score â€” å…¬é–‹ API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compute_accel_score(
    features: pd.DataFrame,
    base_direction: pd.Series,
    params: dict | None = None,
) -> pd.Series:
    """
    å°‡å¾®çµæ§‹ç‰¹å¾µåˆæˆç‚ºã€ŒåŠ é€Ÿ/æ¸›é€Ÿã€åˆ†æ•¸

    Score å«ç¾©ï¼š
        > 0: å¾®çµæ§‹æ”¯æŒç•¶å‰è¶¨å‹¢æ–¹å‘ â†’ åŠ é€Ÿ
        < 0: å¾®çµæ§‹åå°ç•¶å‰è¶¨å‹¢æ–¹å‘ â†’ æ¸›é€Ÿ
        â‰ˆ 0: ä¸­æ€§

    è¨ˆç®—æ–¹å¼ï¼š
        1. ç”¨ base_direction sign å°é½Šç‰¹å¾µæ–¹å‘
           (åšå¤šæ™‚ taker_imbalance > 0 = é †å‹¢ â†’ positive contribution)
        2. åŠ æ¬Šçµ„åˆ
        3. Clip to [-1, 1]

    Args:
        features: compute_micro_features() çš„è¼¸å‡º
        base_direction: 1h åŸºç¤ç­–ç•¥çš„ position sign (+1/-1/0)
        params: scoring weights and thresholds

    Returns:
        accel_score [-1, 1]
    """
    p = params or {}

    # Weights for each feature
    w_taker = float(p.get("w_taker", 0.35))
    w_vol = float(p.get("w_vol", 0.15))
    w_slope = float(p.get("w_slope", 0.25))
    w_burst = float(p.get("w_burst", 0.15))
    w_oi = float(p.get("w_oi", 0.10))

    # Direction alignment: multiply features by sign(base_position)
    # so "confirming" features become positive
    direction = np.sign(base_direction).fillna(0.0)

    # Taker imbalance: positive when aligned with position direction
    taker = features.get("taker_imbalance", pd.Series(0.0, index=features.index))
    taker_aligned = taker * direction

    # Vol regime: complex â€” moderate vol expansion during trend = good,
    # extreme vol = caution. Use inverted-U: best at z âˆˆ [0.5, 1.5]
    vol_z = features.get("vol_regime_z", pd.Series(0.0, index=features.index))
    # Convert to signal: moderate expansion = +, extreme = -
    vol_signal = pd.Series(0.0, index=features.index)
    vol_signal[vol_z.between(0.3, 2.0)] = 1.0  # healthy expansion
    vol_signal[vol_z > 3.0] = -1.0  # extreme â€” caution
    vol_signal[vol_z < -1.0] = -0.5  # low vol â€” weak signal

    # EMA slope: positive when aligned with direction
    slope_z = features.get("ema_slope_z", pd.Series(0.0, index=features.index))
    slope_aligned = slope_z.clip(-2, 2) / 2.0 * direction  # normalize to [-1, 1]

    # Return burst: positive when aligned with direction
    burst_z = features.get("return_burst_z", pd.Series(0.0, index=features.index))
    burst_aligned = burst_z.clip(-2, 2) / 2.0 * direction

    # OI change: positive OI + aligned direction = conviction
    oi_z = features.get("oi_change_z", pd.Series(0.0, index=features.index))
    oi_signal = oi_z.clip(-2, 2) / 2.0  # direction-neutral for now

    # Weighted sum
    total_weight = w_taker + w_vol + w_slope + w_burst + w_oi
    if total_weight <= 0:
        total_weight = 1.0

    score = (
        w_taker * taker_aligned
        + w_vol * vol_signal
        + w_slope * slope_aligned
        + w_burst * burst_aligned
        + w_oi * oi_signal
    ) / total_weight

    score = score.clip(-1.0, 1.0).fillna(0.0)

    # When base position is flat (0), accel score should be 0
    score[direction == 0] = 0.0

    logger.info(
        f"ğŸ“Š Accel score: mean={score.mean():.4f}, std={score.std():.4f}, "
        f"pos_pct={( score > 0.1 ).mean()*100:.1f}%, neg_pct={( score < -0.1 ).mean()*100:.1f}%"
    )

    return score


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  apply_accel_overlay â€” å…¬é–‹ API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def apply_accel_overlay(
    base_position: pd.Series,
    accel_score: pd.Series,
    params: dict | None = None,
) -> pd.Series:
    """
    æ ¹æ“š accel_score èª¿æ•´åŸºç¤å€‰ä½

    è¦å‰‡ï¼š
        1. accel_score > accel_threshold â†’ boost position
           new_size = base * (1 + boost_pct * score)ï¼Œcapped at size_cap
        2. accel_score < -decel_threshold â†’ reduce position
           new_size = base * (1 - reduce_pct * |score|)ï¼Œfloored at size_floor
        3. |accel_score| <= threshold â†’ no change
        4. adverse_micro_exit: if score < -adverse_threshold and position
           has been held for >= min_hold_bars â†’ rapid exit

    Cooldown:
        After any accel/decel action, wait cooldown_bars before next action.

    Anti-lookahead:
        - accel_score[i] ç”¨ [0, i] çš„è³‡æ–™è¨ˆç®—
        - çµæœ position[i] åœ¨ bar[i+1] åŸ·è¡Œ

    IMPORTANT:
        This overlay can INCREASE position size (unlike vol_pause which only reduces).
        This is by design for Track A (è¿½æ±‚æ›´é«˜å¹´åŒ–å ±é…¬).

    Args:
        base_position: 1h ç­–ç•¥çš„åŸå§‹ position [-1, 1]
        accel_score: compute_accel_score() çš„è¼¸å‡º [-1, 1]
        params: overlay åƒæ•¸

    Returns:
        adjusted position [-1, 1]
    """
    p = params or {}

    # Thresholds
    accel_threshold = float(p.get("accel_threshold", 0.2))
    decel_threshold = float(p.get("decel_threshold", 0.2))
    adverse_threshold = float(p.get("adverse_threshold", 0.6))

    # Size multipliers
    boost_pct = float(p.get("boost_pct", 0.3))      # max 30% increase
    reduce_pct = float(p.get("reduce_pct", 0.3))     # max 30% decrease
    size_floor = float(p.get("size_floor", 0.1))      # never go below 10%
    size_cap = float(p.get("size_cap", 1.0))           # never exceed 100%

    # Cooldown
    cooldown_bars = int(p.get("cooldown_bars", 3))

    # Adverse micro exit
    adverse_exit_enabled = bool(p.get("adverse_exit_enabled", True))
    adverse_exit_to = float(p.get("adverse_exit_to", 0.0))  # exit to flat

    n = len(base_position)
    base_arr = base_position.values.copy().astype(float)
    score_arr = accel_score.values.copy().astype(float)
    result = base_arr.copy()

    # Stats
    n_boost = 0
    n_reduce = 0
    n_adverse_exit = 0
    cooldown_remaining = 0

    for i in range(n):
        if cooldown_remaining > 0:
            cooldown_remaining -= 1
            continue

        if abs(base_arr[i]) < 0.001:
            # Position is flat â†’ no accel/decel
            result[i] = base_arr[i]
            continue

        s = score_arr[i]
        abs_base = abs(base_arr[i])
        sign_base = np.sign(base_arr[i])

        # â”€â”€ Adverse exit (highest priority) â”€â”€
        if adverse_exit_enabled and s < -adverse_threshold:
            result[i] = sign_base * adverse_exit_to * abs_base
            n_adverse_exit += 1
            cooldown_remaining = cooldown_bars
            continue

        # â”€â”€ Boost (trend-confirming micro) â”€â”€
        if s > accel_threshold:
            strength = (s - accel_threshold) / (1.0 - accel_threshold + 1e-9)
            new_abs = abs_base * (1.0 + boost_pct * strength)
            new_abs = min(new_abs, size_cap)
            result[i] = sign_base * new_abs
            n_boost += 1
            cooldown_remaining = cooldown_bars
            continue

        # â”€â”€ Reduce (trend-weakening micro) â”€â”€
        if s < -decel_threshold:
            strength = (-s - decel_threshold) / (1.0 - decel_threshold + 1e-9)
            new_abs = abs_base * (1.0 - reduce_pct * strength)
            new_abs = max(new_abs, size_floor * abs_base) if size_floor > 0 else max(new_abs, 0.0)
            result[i] = sign_base * new_abs
            n_reduce += 1
            cooldown_remaining = cooldown_bars
            continue

        # â”€â”€ Neutral: keep original â”€â”€
        result[i] = base_arr[i]

    # Final clip
    result = np.clip(result, -1.0, 1.0)

    # Stats
    pct_boost = n_boost / max(n, 1) * 100
    pct_reduce = n_reduce / max(n, 1) * 100
    pct_adverse = n_adverse_exit / max(n, 1) * 100
    logger.info(
        f"ğŸ“Š Accel Overlay: boost={n_boost} ({pct_boost:.1f}%), "
        f"reduce={n_reduce} ({pct_reduce:.1f}%), "
        f"adverse_exit={n_adverse_exit} ({pct_adverse:.1f}%), "
        f"cooldown={cooldown_bars}b"
    )

    return pd.Series(result, index=base_position.index)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Convenience: Full Pipeline
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def apply_full_micro_accel_overlay(
    base_position: pd.Series,
    df_1h: pd.DataFrame,
    df_5m: pd.DataFrame | None = None,
    df_15m: pd.DataFrame | None = None,
    oi_series: pd.Series | None = None,
    params: dict | None = None,
) -> pd.Series:
    """
    å®Œæ•´å¾®çµæ§‹åŠ é€Ÿ overlay æµç¨‹

    1. compute_micro_features
    2. compute_accel_score
    3. apply_accel_overlay

    Args:
        base_position: 1h ç­–ç•¥çš„åŸå§‹ position [-1, 1]
        df_1h: 1h OHLCV DataFrame
        df_5m: 5m OHLCV DataFrame (optional)
        df_15m: 15m OHLCV DataFrame (optional)
        oi_series: OI series (optional)
        params: all overlay parameters (feature + scoring + overlay)

    Returns:
        adjusted position [-1, 1]
    """
    p = params or {}

    # Feature params
    feature_params = {
        k: p[k] for k in [
            "taker_window", "vol_short_window", "vol_long_window",
            "ema_slope_period", "ema_slope_norm_window",
            "return_burst_window", "oi_lookback", "oi_z_window",
        ] if k in p
    }

    # Scoring params
    scoring_params = {
        k: p[k] for k in [
            "w_taker", "w_vol", "w_slope", "w_burst", "w_oi",
        ] if k in p
    }

    # Overlay params
    overlay_params = {
        k: p[k] for k in [
            "accel_threshold", "decel_threshold", "adverse_threshold",
            "boost_pct", "reduce_pct", "size_floor", "size_cap",
            "cooldown_bars", "adverse_exit_enabled", "adverse_exit_to",
        ] if k in p
    }

    features = compute_micro_features(
        df_1h=df_1h,
        df_5m=df_5m,
        df_15m=df_15m,
        oi_series=oi_series,
        params=feature_params,
    )

    accel_score = compute_accel_score(
        features=features,
        base_direction=base_position,
        params=scoring_params,
    )

    return apply_accel_overlay(
        base_position=base_position,
        accel_score=accel_score,
        params=overlay_params,
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Data Helper: Load multi-timeframe klines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_multi_tf_klines(
    data_dir: Path,
    symbol: str,
    market_type: str = "futures",
) -> dict[str, pd.DataFrame | None]:
    """
    è¼‰å…¥å¤šæ™‚é–“æ¡†æ¶ K ç·šè³‡æ–™

    Returns:
        {
            "1h": df_1h,
            "5m": df_5m or None,
            "15m": df_15m or None,
        }
    """
    from ...data.storage import load_klines as _load

    result = {}
    for tf in ["1h", "5m", "15m"]:
        path = data_dir / "binance" / market_type / tf / f"{symbol}.parquet"
        if path.exists():
            try:
                result[tf] = _load(path)
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to load {tf} data for {symbol}: {e}")
                result[tf] = None
        else:
            result[tf] = None

    return result
