"""
Incremental K-Line Cache â€” å¢é‡ K ç·šå¿«å–

è§£æ±ºå•é¡Œï¼š
    å›æ¸¬ä½¿ç”¨å®Œæ•´æ­·å²æ•¸æ“šï¼ˆå¾ç¬¬ 1 bar åˆ°æœ€å¾Œï¼‰ï¼Œç­–ç•¥çš„ç‹€æ…‹æ©Ÿå¾ bar 0 é–‹å§‹èµ°ã€‚
    å¯¦ç›¤åŸæœ¬æ¯æ¬¡åªæ‹‰æœ€è¿‘ 300 barï¼ˆæ»‘å‹•çª—å£ï¼‰ï¼Œçª—å£åç§» 1 bar å°±å¯èƒ½è®“
    ç‹€æ…‹æ©Ÿèµ°å‘å®Œå…¨ä¸åŒçš„è·¯å¾‘ï¼Œå°è‡´ä¿¡è™Ÿä¸ä¸€è‡´ã€‚

æ–¹æ¡ˆï¼š
    é¦–æ¬¡å•Ÿå‹•æ‹‰å– seed_bars æ ¹ K ç·šä½œç‚ºç¨®å­ï¼Œå­˜å…¥æœ¬åœ° Parquetã€‚
    å¾ŒçºŒæ¯æ¬¡ cron åªæ‹‰ã€Œè‡ªå¿«å–æœ€å¾Œä¸€æ ¹ä»¥ä¾†çš„æ–° K ç·šã€ä¸¦ appendã€‚
    ç­–ç•¥å¾ bar 0 è·‘åˆ°æœ€æ–° bar â†’ èˆ‡å›æ¸¬è¡Œç‚ºä¸€è‡´ã€‚

æ ¼å¼ï¼š
    cache/{symbol}.parquet â€” åƒ…å«å·²æ”¶ç›¤ K ç·šï¼ˆOHLCVï¼‰
    close_time ç”¨æ–¼éæ¿¾æœªæ”¶ç›¤ bar å¾Œå³åˆ»ç§»é™¤ï¼Œè¨˜æ†¶é«”ä¸­ä¸ä¿ç•™ã€‚

å…¸å‹å¤§å°ï¼š
    1h K ç·š Ã— 1 å¹´ â‰ˆ 8,760 bar Ã— ~50 bytes â‰ˆ 430 KBï¼ˆå¯å¿½ç•¥ï¼‰

è¨˜æ†¶é«”ç®¡ç†ï¼š
    max_bars åƒæ•¸é™åˆ¶å¿«å–ä¿ç•™çš„æœ€å¤§ bar æ•¸ï¼ˆé è¨­ 1000ï¼‰ã€‚
    è¶…éæ™‚è‡ªå‹•è£å‰ªæœ€èˆŠçš„ barï¼Œé¿å…é•·æœŸé‹è¡Œ OOMã€‚
    ç”Ÿç”¢ç­–ç•¥æœ€é•· lookback ç´„ 200 barï¼ˆTSMOM 168h + EMA warmupï¼‰ï¼Œ
    1000 bar ç¶½ç¶½æœ‰é¤˜ã€‚
"""
from __future__ import annotations

from datetime import datetime, timezone, timedelta
from pathlib import Path

import pandas as pd

from ..data.klines import fetch_klines, FUTURES_BASE_URL, KLINE_COLS
from ..data.binance_client import BinanceHTTP
from ..data.quality import clean_data
from ..utils.log import get_logger

logger = get_logger("kline_cache")


class IncrementalKlineCache:
    """
    å¢é‡ K ç·šå¿«å–

    Usage:
        cache = IncrementalKlineCache(cache_dir, interval="1h")
        df = cache.get_klines("BTCUSDT")   # é¦–æ¬¡æ‹‰ 300 barï¼Œå¾ŒçºŒåªæ‹‰å¢é‡
        # df æœƒè¶Šä¾†è¶Šé•·ï¼Œç­‰æ•ˆå›æ¸¬çš„å®Œæ•´æ­·å²
    """

    def __init__(
        self,
        cache_dir: Path,
        interval: str = "1h",
        seed_bars: int = 300,
        market_type: str = "futures",
        max_bars: int = 1000,
    ):
        """
        Args:
            cache_dir:    å¿«å–ç›®éŒ„ï¼Œä¾‹å¦‚ reports/futures/rsi_adx_atr/live/kline_cache/
            interval:     K ç·šé€±æœŸï¼Œä¾‹å¦‚ "1h"
            seed_bars:    é¦–æ¬¡æ‹‰å–çš„ K ç·šæ•¸é‡ï¼ˆç¨®å­ï¼‰
            market_type:  "spot" æˆ– "futures"
            max_bars:     è¨˜æ†¶é«”ä¸­ä¿ç•™çš„æœ€å¤§ bar æ•¸é‡ï¼ˆé˜²æ­¢ç„¡é™å¢é•·å°è‡´ OOMï¼‰
                          è¨­ç‚º 0 æˆ– None å‰‡ä¸é™åˆ¶
        """
        self.cache_dir = Path(cache_dir)
        self.interval = interval
        self.seed_bars = seed_bars
        self.market_type = market_type
        self.max_bars = max_bars or 0

        # è¨˜æ†¶é«”å¿«å–ï¼ˆé¿å…æ¯æ¬¡éƒ½è®€ Parquetï¼‰
        self._mem_cache: dict[str, pd.DataFrame] = {}

        # interval â†’ åˆ†é˜
        self._interval_minutes = {
            "1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30,
            "1h": 60, "2h": 120, "4h": 240, "6h": 360, "8h": 480,
            "12h": 720, "1d": 1440,
        }.get(interval, 60)

    # â”€â”€ å…¬é–‹ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_klines(self, symbol: str) -> pd.DataFrame:
        """
        å–å¾—å®Œæ•´çš„å·²æ”¶ç›¤ K ç·šï¼ˆå«æ­·å²å¿«å– + æœ€æ–°å¢é‡ï¼‰

        é¦–æ¬¡å‘¼å« â†’ æ‹‰ seed_bars æ ¹ï¼Œå­˜å…¥å¿«å–
        å¾ŒçºŒå‘¼å« â†’ å¾å¿«å–æœ€å¾Œä¸€æ ¹å¾€å¾Œæ‹‰æ–°çš„ barï¼Œappend

        Returns:
            DataFrame (index=open_time UTC, cols=[open, high, low, close, volume])
        """
        cached = self._load(symbol)

        if cached is not None and len(cached) > 0:
            # â”€â”€ å¢é‡æ›´æ–° â”€â”€
            new_bars = self._fetch_since(symbol, cached.index[-1])

            if new_bars is not None and len(new_bars) > 0:
                combined = pd.concat([cached, new_bars])
                combined = combined[~combined.index.duplicated(keep="last")]
                combined = combined.sort_index()
                combined = self._drop_unclosed(combined)
                combined = clean_data(
                    combined,
                    fill_method="forward",
                    remove_outliers=False,
                    remove_duplicates=True,
                )
                self._save(symbol, combined)
                logger.info(
                    f"ğŸ“¦ {symbol}: å¿«å–å¢é‡æ›´æ–° +{len(new_bars)} bar "
                    f"â†’ ç¸½è¨ˆ {len(combined)} bar "
                    f"({combined.index[0].strftime('%Y-%m-%d')} ~ "
                    f"{combined.index[-1].strftime('%Y-%m-%d %H:%M')})"
                )
                return combined
            else:
                logger.debug(f"  {symbol}: å¿«å–å·²æ˜¯æœ€æ–° ({len(cached)} bar)")
                return cached
        else:
            # â”€â”€ é¦–æ¬¡å•Ÿå‹•ï¼šæ‹‰å–ç¨®å­æ•¸æ“š â”€â”€
            seed = self._fetch_seed(symbol)
            if seed is not None and len(seed) > 0:
                self._save(symbol, seed)
                logger.info(
                    f"ğŸŒ± {symbol}: é¦–æ¬¡å»ºç«‹å¿«å– {len(seed)} bar "
                    f"({seed.index[0].strftime('%Y-%m-%d')} ~ "
                    f"{seed.index[-1].strftime('%Y-%m-%d %H:%M')})"
                )
            else:
                logger.warning(f"âš ï¸  {symbol}: ç„¡æ³•å–å¾—ç¨®å­æ•¸æ“š")
            return seed if seed is not None else pd.DataFrame()

    def get_bar_count(self, symbol: str) -> int:
        """å–å¾—å¿«å–ä¸­çš„ bar æ•¸é‡ï¼ˆä¸è§¸ç™¼æ›´æ–°ï¼‰"""
        cached = self._load(symbol)
        return len(cached) if cached is not None else 0

    def clear(self, symbol: str | None = None) -> None:
        """æ¸…é™¤å¿«å–ï¼ˆsymbol=None æ¸…å…¨éƒ¨ï¼‰"""
        if symbol:
            path = self._cache_path(symbol)
            if path.exists():
                path.unlink()
            self._mem_cache.pop(symbol, None)
            logger.info(f"ğŸ—‘ï¸  {symbol}: å¿«å–å·²æ¸…é™¤")
        else:
            if self.cache_dir.exists():
                for f in self.cache_dir.glob("*.parquet"):
                    f.unlink()
            self._mem_cache.clear()
            logger.info("ğŸ—‘ï¸  æ‰€æœ‰å¿«å–å·²æ¸…é™¤")

    # â”€â”€ WebSocket æ•´åˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_cached(self, symbol: str) -> pd.DataFrame | None:
        """
        å–å¾—è¨˜æ†¶é«”ä¸­çš„å¿«å–æ•¸æ“šï¼ˆä¸è§¸ç™¼ HTTP æ›´æ–°ï¼‰

        é©ç”¨æ–¼ WebSocket æ¨¡å¼ï¼šç”± WS è² è²¬å¢é‡æ›´æ–°ï¼Œç­–ç•¥è®€å–æ™‚ä¸éœ€è¦ HTTPã€‚

        Returns:
            DataFrame or None if no cache exists
        """
        return self._load(symbol)

    def append_bar(self, symbol: str, bar_df: pd.DataFrame) -> pd.DataFrame:
        """
        è¿½åŠ å–®æ ¹ K ç·šåˆ°å¿«å–ï¼ˆWebSocket ç”¨ï¼‰

        ä¸åš HTTP è«‹æ±‚ï¼Œç›´æ¥è¿½åŠ åˆ°è¨˜æ†¶é«” + ç£ç¢Ÿå¿«å–ã€‚
        èˆ‡ get_klines() çš„ HTTP å¢é‡æ›´æ–°äº’è£œã€‚

        Args:
            symbol:  äº¤æ˜“å°
            bar_df:  å–®è¡Œ DataFrame (index=open_time UTC,
                     cols=[open, high, low, close, volume] + optional close_time)

        Returns:
            æ›´æ–°å¾Œçš„å®Œæ•´ DataFrame
        """
        cached = self._load(symbol)

        # ç¢ºä¿ UTC index
        if bar_df.index.tz is None:
            bar_df.index = bar_df.index.tz_localize("UTC")

        # WS bar å·²ç¢ºèªæ”¶ç›¤ï¼Œç§»é™¤ close_timeï¼ˆè¨˜æ†¶é«”ä¸­ä¸ä¿ç•™ï¼‰
        if "close_time" in bar_df.columns:
            bar_df = bar_df.drop(columns=["close_time"])

        if cached is not None and len(cached) > 0:
            combined = pd.concat([cached, bar_df])
            combined = combined[~combined.index.duplicated(keep="last")]
            combined = combined.sort_index()
        else:
            combined = bar_df

        self._save(symbol, combined)
        logger.debug(f"  {symbol}: è¿½åŠ  1 bar â†’ ç¸½è¨ˆ {len(combined)} bar")
        return combined

    def fill_gap(self, symbol: str, last_cached_time: pd.Timestamp) -> pd.DataFrame | None:
        """
        è£œé½Šå¿«å–ç¼ºå£ï¼ˆWebSocket æ–·ç·šé‡é€£å¾Œä½¿ç”¨ï¼‰

        å¾ last_cached_time å¾€å¾Œæ‹‰å–éºæ¼çš„ K ç·šã€‚

        Returns:
            æ›´æ–°å¾Œçš„å®Œæ•´ DataFrame, æˆ– None å¦‚æœå¤±æ•—
        """
        try:
            new_bars = self._fetch_since(symbol, last_cached_time)
            if new_bars is not None and len(new_bars) > 0:
                cached = self._load(symbol)
                if cached is not None and len(cached) > 0:
                    combined = pd.concat([cached, new_bars])
                    combined = combined[~combined.index.duplicated(keep="last")]
                    combined = combined.sort_index()
                    combined = self._drop_unclosed(combined)
                    self._save(symbol, combined)
                    logger.info(
                        f"ğŸ“¦ {symbol}: è£œé½Šç¼ºå£ +{len(new_bars)} bar â†’ ç¸½è¨ˆ {len(combined)} bar"
                    )
                    return combined
            return self._load(symbol)
        except Exception as e:
            logger.warning(f"âš ï¸  {symbol}: è£œé½Šç¼ºå£å¤±æ•—: {e}")
            return self._load(symbol)

    # â”€â”€ å…§éƒ¨æ–¹æ³• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _cache_path(self, symbol: str) -> Path:
        return self.cache_dir / f"{symbol}.parquet"

    def _load(self, symbol: str) -> pd.DataFrame | None:
        """å¾è¨˜æ†¶é«”å¿«å–æˆ–ç£ç¢Ÿ Parquet è¼‰å…¥"""
        # è¨˜æ†¶é«”å¿«å–
        if symbol in self._mem_cache:
            return self._mem_cache[symbol]

        # ç£ç¢Ÿ
        path = self._cache_path(symbol)
        if not path.exists():
            return None

        try:
            df = pd.read_parquet(path)

            # ç¢ºä¿ index æ˜¯ DatetimeIndex (UTC)
            if not isinstance(df.index, pd.DatetimeIndex):
                if "open_time" in df.columns:
                    df = df.set_index("open_time")
                df.index = pd.to_datetime(df.index, utc=True)

            if df.index.tz is None:
                df.index = df.index.tz_localize("UTC")

            # å‘å¾Œç›¸å®¹ï¼šèˆŠ parquet å¯èƒ½é‚„æœ‰ close_timeï¼Œè¼‰å…¥è¨˜æ†¶é«”æ™‚ç§»é™¤
            if "close_time" in df.columns:
                df = df.drop(columns=["close_time"])

            self._mem_cache[symbol] = df
            logger.debug(f"  {symbol}: å¾ç£ç¢Ÿè¼‰å…¥å¿«å– {len(df)} bar")
            return df
        except Exception as e:
            logger.warning(f"âš ï¸  {symbol}: è¼‰å…¥å¿«å–å¤±æ•—: {e}")
            return None

    def _save(self, symbol: str, df: pd.DataFrame) -> None:
        """ä¿å­˜åˆ°è¨˜æ†¶é«”å¿«å–å’Œç£ç¢Ÿ Parquetï¼ˆè¶…é max_bars æ™‚è£å‰ªèˆŠè³‡æ–™ï¼‰"""
        if self.max_bars > 0 and len(df) > self.max_bars:
            trimmed = len(df) - self.max_bars
            df = df.iloc[-self.max_bars:]
            logger.debug(f"  {symbol}: è£å‰ªå¿«å– -{trimmed} bar â†’ ä¿ç•™ {self.max_bars} bar")

        self._mem_cache[symbol] = df

        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            df.to_parquet(self._cache_path(symbol))
        except Exception as e:
            logger.warning(f"âš ï¸  {symbol}: ä¿å­˜å¿«å–å¤±æ•—: {e}")

    def _fetch_seed(self, symbol: str) -> pd.DataFrame | None:
        """é¦–æ¬¡å•Ÿå‹•ï¼šæ‹‰å– seed_bars æ ¹å·²æ”¶ç›¤ K ç·š"""
        try:
            start_dt = datetime.now(timezone.utc) - timedelta(
                minutes=self._interval_minutes * (self.seed_bars + 10)
            )
            start_str = start_dt.strftime("%Y-%m-%d")

            df = fetch_klines(
                symbol=symbol,
                interval=self.interval,
                start=start_str,
                market_type=self.market_type,
            )
            df = clean_data(
                df, fill_method="forward",
                remove_outliers=False, remove_duplicates=True,
            )
            df = self._drop_unclosed(df)

            # ç¨®å­åªå–æœ€è¿‘ seed_bars æ ¹
            if len(df) > self.seed_bars:
                df = df.iloc[-self.seed_bars:]

            return df
        except Exception as e:
            logger.error(f"âŒ {symbol}: æ‹‰å–ç¨®å­æ•¸æ“šå¤±æ•—: {e}")
            return None

    def _fetch_since(
        self, symbol: str, last_time: pd.Timestamp,
    ) -> pd.DataFrame | None:
        """å¢é‡æ‹‰å–ï¼šå¾ last_time ä¹‹å¾Œçš„æ–° K ç·š"""
        try:
            # å¾å¿«å–æœ€å¾Œä¸€æ ¹çš„é–‹ç›¤æ™‚é–“å¾€å¾Œæ‹‰
            # +1 ms é¿å…é‡è¤‡æ‹‰æœ€å¾Œä¸€æ ¹
            start_ms = int(last_time.timestamp() * 1000) + 1

            if self.market_type == "futures":
                http = BinanceHTTP(base_url=FUTURES_BASE_URL)
                endpoint = "/fapi/v1/klines"
            else:
                http = BinanceHTTP()
                endpoint = "/api/v3/klines"

            params = {
                "symbol": symbol,
                "interval": self.interval,
                "startTime": start_ms,
                "limit": 1000,
            }

            chunk = http.get(endpoint, params=params)
            if not chunk:
                return None

            df = pd.DataFrame(chunk, columns=KLINE_COLS)
            for c in ["open", "high", "low", "close", "volume"]:
                df[c] = df[c].astype(float)
            df["open_time"] = pd.to_datetime(df["open_time"], unit="ms", utc=True)
            df["close_time"] = pd.to_datetime(df["close_time"], unit="ms", utc=True)
            df = df.set_index("open_time").sort_index()
            df = df[["open", "high", "low", "close", "volume", "close_time"]]

            df = self._drop_unclosed(df)
            return df if len(df) > 0 else None

        except Exception as e:
            logger.warning(f"âš ï¸  {symbol}: å¢é‡æ‹‰å–å¤±æ•—: {e}")
            return None

    @staticmethod
    def _drop_unclosed(df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸Ÿæ£„æœªæ”¶ç›¤çš„ K ç·šï¼Œç„¶å¾Œç§»é™¤ close_time æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”ã€‚

        ç­–ç•¥ä¸éœ€è¦ close_timeï¼ˆåªç”¨ OHLCVï¼‰ï¼Œæ­¤æ¬„ä½åƒ…ç”¨æ–¼åˆ¤æ–·æ˜¯å¦å·²æ”¶ç›¤ã€‚
        ç§»é™¤å¾Œæ¯å€‹ symbol å¯ç¯€çœ ~8 bytes/bar çš„ datetime64 è¨˜æ†¶é«”ã€‚
        """
        if len(df) == 0:
            return df
        if "close_time" not in df.columns:
            return df
        now = pd.Timestamp.now(tz="UTC")
        # ä¿ç•™ close_time ç‚º NaN çš„è¡Œï¼ˆèˆŠå¿«å–å·²ç§»é™¤ close_time çš„è³‡æ–™ï¼‰
        has_ct = df["close_time"].notna()
        unclosed = has_ct & (df["close_time"] > now)
        n_dropped = unclosed.sum()
        if n_dropped > 0:
            logger.debug(f"  ä¸Ÿæ£„ {n_dropped} æ ¹æœªæ”¶ç›¤ K ç·š")
        df = df[~unclosed]
        # ç§»é™¤ close_time â€” ç­–ç•¥ä¸éœ€è¦ï¼Œç¯€çœè¨˜æ†¶é«”
        df = df.drop(columns=["close_time"], errors="ignore")
        return df
